{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retail Revenue Prediction - Short Notebook for Kaggle Submission:** *pred_new_stripped_150outliers_3feat.csv*\n",
    "\n",
    "##### **Group Information**\n",
    "**Participants:**\n",
    "- Mina Chen Glein Feragen - 544552\n",
    "- Andrew Glover Marty - 557813\n",
    "- Simen Tvete Aabol - 505174\n",
    "\n",
    "\n",
    "**Kaggle Team name:** Group 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short about the prediction\n",
    "\n",
    "For this prediction we used gradient boosting, as we found that the algorithm gave accurate predictions and at the same time being easy to implement.\n",
    "\n",
    "As for the features, we first used all of the features we had computed at the time, and then plotted the importance of each feature, as can be seen in the long notebook. Based on these foundings, we chose to \"strip\" away 'lv1', 'busstop_importance_level', 'municipality_name', 'distance_from_busstop', 'district_municipality_name', since these were the least important according to the algorithm itself. We did however also make predictions with all of the features, which can be seen commented out throughout this notebook, but we chose to go with the \"stripped\" version, as it had performed better with other combinations as well, and we wanted to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/lhome/mcferage/machine-learning-fall2022/short_notebook_pred_new_stripped_150outliers_3feat.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btdt4173-08.idi.ntnu.no/lhome/mcferage/machine-learning-fall2022/short_notebook_pred_new_stripped_150outliers_3feat.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m stores_train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdata/stores_train.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To prepare for the feature engineering, we replaced categorical values in the busstops-table with numerical ones,\n",
    "and made own columns for latitude and longditude, so that computing the distance to a busstop would be \n",
    "easier in the future.\n",
    "\"\"\"\n",
    "\n",
    "busstops = pd.read_csv('./data/busstops_norway.csv')\n",
    "busstops = busstops.drop(columns=[\"stopplace_type\", \"side_placement\"])\n",
    "\n",
    "importance_levels = [\"Mangler viktighetsniv√•\", \"Standard holdeplass\", \"Lokalt knutepunkt\",\n",
    "                     \"Regionalt knutepunkt\", \"Annen viktig holdeplass\", \"Nasjonalt knutepunkt\"]\n",
    "numerated_importance_levels = [1, 2, 4, 5, 3, 6]\n",
    "\n",
    "busstops[\"importance_level\"] = busstops[\"importance_level\"].replace(importance_levels, numerated_importance_levels)\n",
    "\n",
    "busstops.insert(3, \"lat\", -math.inf)\n",
    "busstops.insert(4, \"lon\", -math.inf)\n",
    "\n",
    "busstops_array = []\n",
    "for row_index in range(len(busstops)):\n",
    "    coordinates = busstops[\"geometry\"][row_index][6:-2].split(' ')\n",
    "    busstops[\"lon\"][row_index] = float(coordinates[0])\n",
    "    busstops[\"lat\"][row_index] = float(coordinates[1])\n",
    "    busstops_array.append([float(coordinates[1]), float(coordinates[0])])\n",
    "busstops = busstops.drop(columns=\"geometry\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also wanted to add other data from the given csv-files, which is added to the stores_train-DataFrame by this function\n",
    "\n",
    "def addStoreData(df):\n",
    "    df.pop(\"sales_channel_name\")\n",
    "    df.pop(\"address\")\n",
    "    df.pop(\"year\")\n",
    "    df.pop(\"store_name\")\n",
    "    \n",
    "    try:\n",
    "        try_accessing = df[\"revenue\"]\n",
    "        df = copy.deepcopy(df[df.revenue != 0])\n",
    "        df.at[7435, 'chain_name'] = np.nan \n",
    "        df.at[8164, 'chain_name'] = \"KIWI NORGE\"\n",
    "    except KeyError:\n",
    "        print(\" Revenue doesn't exist in dataframe df\")\n",
    " \n",
    "    plaace_hierarchy = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "    plaace_hierarchy.pop('lv1_desc')\n",
    "    plaace_hierarchy.pop('lv2_desc')\n",
    "    plaace_hierarchy.pop('lv3_desc')\n",
    "    plaace_hierarchy.pop('lv4_desc')\n",
    "    plaace_hierarchy.pop('sales_channel_name')\n",
    "    df = pd.merge(df, plaace_hierarchy, how='left', on='plaace_hierarchy_id')\n",
    "    df.pop('plaace_hierarchy_id') \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index, 'lv2'] = int(str(df.loc[index, 'lv2']).replace('.', ''))\n",
    "        df.at[index, 'lv3'] = int(df.loc[index, 'lv3'].replace('.', ''))\n",
    "        df.at[index, 'lv4'] = int(df.loc[index, 'lv4'].replace('.', ''))\n",
    "\n",
    "    \n",
    "    districts = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "    districts.pop(\"year\")\n",
    "    districts.pop('grunnkrets_name')\n",
    "    districts.pop(\"area_km2\")\n",
    "    districts = districts.drop_duplicates()\n",
    "    districts = districts.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    grunnkrets_household_income = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "    grunnkrets_household_income.head()\n",
    "    grunnkrets_household_income.pop(\"singles\")\n",
    "    grunnkrets_household_income.pop(\"couple_without_children\")\n",
    "    grunnkrets_household_income.pop(\"couple_with_children\")\n",
    "    grunnkrets_household_income.pop(\"other_households\")\n",
    "    grunnkrets_household_income.pop(\"single_parent_with_children\")\n",
    "    grunnkrets_household_income = grunnkrets_household_income[grunnkrets_household_income.year != 2015]\n",
    "    grunnkrets_household_income.pop(\"year\")\n",
    "    \n",
    "    income_districts = pd.merge(grunnkrets_household_income,districts, how = 'right', on = 'grunnkrets_id')\n",
    "    income_districts.rename(columns = {'all_households':'households_grunnkrets'}, inplace = True)\n",
    "    \n",
    "    df = pd.merge(df,income_districts, how = 'left', on = 'grunnkrets_id')\n",
    "    \n",
    "\n",
    "    # Mall name\n",
    "    df['mall_name'] = df['mall_name'].replace(np.nan, 0)\n",
    "    for i in range(len(df) ):\n",
    "        if( df['mall_name'][i] != 0):\n",
    "            df.loc[i,'mall_name'] = 1\n",
    "    df['mall_name'] = pd.to_numeric(df['mall_name'])\n",
    "            \n",
    "            \n",
    "         \n",
    "    # Chain name  \n",
    "    df['chain_name'] = df['chain_name'].replace(np.nan, \"A-not a chain\") \n",
    "    threshold = 6 # Anything that occurs less than this will be removed.\n",
    "    for col in df.columns:\n",
    "        value_counts = df['chain_name'].value_counts() # Specific column \n",
    "        to_remove = value_counts[value_counts <= threshold].index\n",
    "        df['chain_name'].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "        \n",
    "    df['chain_name'] = LabelEncoder().fit_transform(df['chain_name'])\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    # code that handle missing data\n",
    "        \n",
    "    df['geometry'] = gpd.GeoSeries.from_wkt(df['geometry'])\n",
    "    store_gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    store_gdf = store_gdf.drop_duplicates()\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        pattern = r'[0-9]'\n",
    "        \n",
    "        if pd.isnull(row['municipality_name']):\n",
    "            lat = row['lat']\n",
    "            lon = row['lon']\n",
    "            \n",
    "            store_location = Point(lon, lat)\n",
    "        \n",
    "            polygon_indices_municipality_name = store_gdf.distance(store_location).sort_values().index[0:1]\n",
    "            nearest_grunnkretser = store_gdf.loc[polygon_indices_municipality_name]\n",
    "\n",
    "                    \n",
    "            pair_value = nearest_grunnkretser['municipality_name'].sort_values()\n",
    "            one_pair = pair_value[0:1]\n",
    "            str_pair = str(one_pair)\n",
    "            str_pair = re.sub(pattern, '', str_pair)        \n",
    "            word = str_pair.split()[0]\n",
    "            df.at[index, 'municipality_name'] = word\n",
    "        \n",
    "        if pd.isnull(row['district_name']):\n",
    "            lat = row['lat']\n",
    "            lon = row['lon']\n",
    "            \n",
    "            store_location = Point(lon, lat)\n",
    "        \n",
    "            polygon_indices_district_name = store_gdf.distance(store_location).sort_values().index[0:1]\n",
    "            nearest_grunnkretser = store_gdf.loc[polygon_indices_district_name]\n",
    "            \n",
    "            pair_value = nearest_grunnkretser['district_name'].sort_values()\n",
    "            one_pair = pair_value[0:1]\n",
    "            str_pair = str(one_pair)\n",
    "            str_pair = re.sub(pattern, '', str_pair)        \n",
    "            word = str_pair.split()[0]\n",
    "            df.loc[index, 'district_name'] = word\n",
    "            \n",
    "        if pd.isnull(row['households_grunnkrets']):\n",
    "            lat = row['lat']\n",
    "            lon = row['lon']\n",
    "            \n",
    "            store_location = Point(lon, lat)\n",
    "        \n",
    "            polygon_indices_all_households = store_gdf.distance(store_location).sort_values().index[0:4]\n",
    "            nearest_grunnkretser = store_gdf.loc[polygon_indices_all_households]\n",
    "            \n",
    "            df.loc[index, 'households_grunnkrets'] = nearest_grunnkretser['households_grunnkrets'].mean()\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "    df['district_municipality_name'] = df['district_name'] + '-'+ df['municipality_name']\n",
    "    df['district_municipality_name'] = LabelEncoder().fit_transform(df['district_municipality_name'])\n",
    "    df['municipality_name'] = LabelEncoder().fit_transform(df['municipality_name'])\n",
    "    \n",
    "\n",
    "    df.pop('geometry')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Back to the busstops, we wanted to compute the distance to the closest busstop, and also what we called \"weighted bus data\", which computes\n",
    "the product of the busstop importance level and the distance, and takes the 10-log of that, which we found to be the most effective.\n",
    "\"\"\"\n",
    "\n",
    "busstops_array = []\n",
    "for row_index in range(len(busstops)):\n",
    "    busstops_array.append([busstops[\"lat\"][row_index], busstops[\"lon\"][row_index]])\n",
    "    \n",
    "# Defining a function that finds the closest busstop and the distance\n",
    "def getDistFromBusStop(store_lat, store_lon):\n",
    "        busstops_big_array = np.asarray(busstops_array)\n",
    "        distances = np.sum((busstops_big_array-[store_lat, store_lon])**2, axis=1)\n",
    "        index_busstop = np.argmin(distances)\n",
    "        shortest_distance = distances[index_busstop]\n",
    "        return shortest_distance, busstops.at[index_busstop, \"importance_level\"]\n",
    "\n",
    "def addWeightedBusData(df):\n",
    "    for index, row in df.iterrows():\n",
    "         # Adding distance to closest busstop\n",
    "        distance, importance_level = getDistFromBusStop(row['lat'], row['lon'])\n",
    "        df.at[index, \"distance_from_busstop\"] = distance\n",
    "        df.at[index, \"busstop_importance_level\"]= importance_level\n",
    "        \n",
    "    df.insert(11, 'buss', -math.inf)\n",
    "    df['buss'] = np.log10(np.multiply(df['distance_from_busstop'], df['busstop_importance_level']))\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPopulation(stores_data):\n",
    "    \"\"\" Returns population of grunnkrets and district\n",
    "        Manipulate or remove last line of code according to your taste :)\n",
    "        May require some more work for imputation of 'grunnkrets_population' using data from 'stores_extra' using 'lat' and 'lon'\n",
    "        Possibility of using unspervised learning?\n",
    "        Can be used for training only.Need to see how to make it multipurpose for test data as well.\n",
    "        Else Separate function is needed.\n",
    "    \"\"\"\n",
    "    input_data = copy.deepcopy(stores_data)\n",
    "    grunnkrets_ages = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "    grunnkrets = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "    #Get data for every grunnkrets_id and and drop duplicates. Prioritize the year '2016'\n",
    "    grunnkrets = grunnkrets.sort_values('year', ascending=False).drop_duplicates('grunnkrets_id').sort_index()\n",
    "    \n",
    "    #Create District+Municipality\n",
    "    grunnkrets_merged_district_municipality_name = grunnkrets\n",
    "    grunnkrets_merged_district_municipality_name['district_name_pro'] = grunnkrets['district_name'] + ' '+ grunnkrets['municipality_name']\n",
    "    \n",
    "    #Drop columns except 'grunnkrets_id' and 'district_municipality_name'\n",
    "    grunnkrets_with_district_municipality_names = grunnkrets_merged_district_municipality_name\n",
    "    grunnkrets_with_district_municipality_names = grunnkrets_with_district_municipality_names.drop(grunnkrets_with_district_municipality_names.iloc[:,1:5].columns,axis =1)\n",
    "    \n",
    "    #Get data for every grunnkrets_id and and drop duplicates. Prioritize the year '2016'\n",
    "    grunnkrets_ages_new = grunnkrets_ages.sort_values('year', ascending=False).drop_duplicates('grunnkrets_id').sort_index()\n",
    "    \n",
    "    #Sum all ages in grunnkrets\n",
    "    grunnkrets_ages_new['population'] = grunnkrets_ages_new.iloc[:,2:].sum(axis =1)\n",
    "    \n",
    "    #Clean: Drop all age columns including year column\n",
    "    grunnkrets_population = grunnkrets_ages_new\n",
    "    grunnkrets_population = grunnkrets_population.drop(grunnkrets_population.iloc[:,1:93].columns,axis =1)\n",
    "    \n",
    "    grunnkrets_population_dist_muni = grunnkrets_population\n",
    "    grunnkrets_population_dist_muni = pd.merge(grunnkrets_with_district_municipality_names,grunnkrets_population, how = 'left', on = 'grunnkrets_id')\n",
    "    \n",
    "    grunnkrets_dist_muni = grunnkrets_population_dist_muni\n",
    "    grunnkrets_dist_muni = grunnkrets_dist_muni.drop(['population'], axis = 1)\n",
    "    \n",
    "    #Merge only grunnkrets population and find missing population before merging\n",
    "    merge_grunnkrets_populn_stores_train = pd.merge(stores_data, grunnkrets_dist_muni, how = 'left', on = 'grunnkrets_id')\n",
    "    \n",
    "    #Add population of district\n",
    "    st_train_grunn_pp = pd.merge(merge_grunnkrets_populn_stores_train,grunnkrets_population, how = 'left', on = 'grunnkrets_id')\n",
    "    \n",
    "    #Get Population of District\n",
    "    population_dist_muni = st_train_grunn_pp\n",
    "    population_dist_muni = population_dist_muni.groupby('district_name_pro')['population'].sum()\n",
    "    \n",
    "    #Merge\n",
    "    st_train_grunn_pp_dist_pp = pd.merge(st_train_grunn_pp,population_dist_muni, how = 'left', on = 'district_name_pro')\n",
    "    \n",
    "    st_train_grunn_pp_dist_pp.rename(columns = {'population_x':'grunnkrets_population','population_y': 'district_population'}, inplace = True)\n",
    "    \n",
    "    st_train_grunn_pp_dist_pp['geometry'] = gpd.GeoSeries.from_wkt(st_train_grunn_pp_dist_pp['geometry'])\n",
    "\n",
    "    store_gdf = gpd.GeoDataFrame(st_train_grunn_pp_dist_pp, geometry='geometry')\n",
    "    store_gdf = store_gdf.drop_duplicates()\n",
    "    \n",
    "    for index, row in st_train_grunn_pp_dist_pp.iterrows():\n",
    "        # print(\"row\", row)\n",
    "    \n",
    "        if pd.isnull(row['grunnkrets_population']):\n",
    "            lat = row['lat']\n",
    "            lon = row['lon']\n",
    "            \n",
    "            store_location = Point(lon, lat)\n",
    "        \n",
    "            polygon_indices = store_gdf.distance(store_location).sort_values().index[0:150] #lower values returns missing data for grunnkrets_population\n",
    "            #cannot guarantee accuracy of imputed missing population\n",
    "            nearest_grunnkretser = store_gdf.loc[polygon_indices]\n",
    "            \n",
    "            st_train_grunn_pp_dist_pp['grunnkrets_population'].loc[index] = np.floor(nearest_grunnkretser['grunnkrets_population'].mean())\n",
    "        \n",
    "    for index, row in st_train_grunn_pp_dist_pp.iterrows():        \n",
    "        if pd.isnull(row['district_population']):\n",
    "            lat = row['lat']\n",
    "            lon = row['lon']\n",
    "            \n",
    "            store_location = Point(lon, lat)\n",
    "        \n",
    "            polygon_indices = store_gdf.distance(store_location).sort_values().index[0:4]\n",
    "            nearest_grunnkretser = store_gdf.loc[polygon_indices]\n",
    "            \n",
    "            st_train_grunn_pp_dist_pp['district_population'].loc[index] = np.floor(nearest_grunnkretser['district_population'].mean())\n",
    "    \n",
    "    st_train_grunn_pp_dist_pp = st_train_grunn_pp_dist_pp.drop(st_train_grunn_pp_dist_pp.iloc[:,1:14].columns,axis =1)#Take off this if all colmuns are needed\n",
    "    \n",
    "    return pd.merge(input_data, st_train_grunn_pp_dist_pp, how=\"left\", on=\"store_id\").drop_duplicates('store_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We thought some revenues would not have a representative revenue, and chose therefore to remove the top and bottom 150, which is ~2.5% of the data\n",
    "def removeNOutliers(df, n = 150):\n",
    "    df = copy.deepcopy(df[df['revenue'] != 0])\n",
    "    df = df.sort_values(by='revenue')\n",
    "    return df[n:-n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To preprocess the data we made a common function for all features and other data preprocessing\n",
    "def preprocess_data(input_data):\n",
    "    \n",
    "    input_data = addPopulation(input_data)\n",
    "    input_data = addStoreData(input_data)\n",
    "    input_data = addWeightedBusData(input_data)\n",
    "    \n",
    "    training_data = removeNOutliers(copy.deepcopy(input_data[input_data['revenue']!=-math.inf]))\n",
    "    test_data = copy.deepcopy(input_data[input_data['revenue']==-math.inf])\n",
    "    \n",
    "    input_data = training_data.append(test_data)\n",
    "    \n",
    "    input_data['district_municipality_name'] = LabelEncoder().fit_transform(input_data['district_municipality_name'])\n",
    "    input_data['municipality_name'] = LabelEncoder().fit_transform(input_data['municipality_name'])\n",
    "    input_data['mall_name'].replace(np.nan(), 0)\n",
    "    \n",
    "    input_data = input_data.drop(columns=[])\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv(\"./data/stores_train.csv\")\n",
    "stores_extra = pd.read_csv(\"./data/stores_extra.csv\")\n",
    "stores_extended = stores_train.append(stores_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_test = pd.read_csv('data/stores_test.csv')\n",
    "stores_test.insert(len(stores_test.columns), column=\"revenue\", value=-math.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stores = pd.DataFrame(stores_extended).append(stores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = preprocess_data(all_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train_preprocessed = copy.deepcopy(preprocessed[preprocessed['revenue']!=-math.inf])\n",
    "stores_test_preprocessed = copy.deepcopy(preprocessed[preprocessed['revenue']==-math.inf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in stores_test_preprocessed.iterrows(): # Finding out levels for the stores who missed it\n",
    "    if (pd.isna(stores_test_preprocessed.loc[index, 'lv4'])):\n",
    "        lv4 = stores_test_preprocessed.loc[index, 'plaace_hierarchy_id'].replace('.', '')\n",
    "        stores_test_preprocessed.at[index, 'lv1'] = int(lv4[0])\n",
    "        stores_test_preprocessed.at[index, 'lv2'] = int(lv4[0:2])\n",
    "        stores_test_preprocessed.at[index, 'lv3'] = int(lv4[0:3])\n",
    "        stores_test_preprocessed.at[index, 'lv4'] = int(lv4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping categorical columns\n",
    "stores_train_preprocessed = stores_train_preprocessed.drop(columns=['store_name', 'plaace_hierarchy_id','sales_channel_name', 'address', 'year'])\n",
    "stores_test_preprocessed = stores_test_preprocessed.drop(columns=['store_name', 'plaace_hierarchy_id','sales_channel_name', 'address', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing NaN-data and transforming ID\n",
    "for row in range(0, len(stores_train_preprocessed)):\n",
    "    stores_train_preprocessed.iloc[row, 0] = int(stores_train_preprocessed.iloc[row, 0].replace('-', ''))\n",
    "\n",
    "stores_train_preprocessed = stores_train_preprocessed[stores_train_preprocessed['district_population'].notna()]\n",
    "stores_train_preprocessed = stores_train_preprocessed[stores_train_preprocessed['grunnkrets_population'].notna()]\n",
    "stores_train_preprocessed = copy.deepcopy(stores_train_preprocessed[stores_train_preprocessed['households_grunnkrets'].notnull()])\n",
    "stores_train_preprocessed = copy.deepcopy(stores_train_preprocessed[stores_train_preprocessed['revenue'].notna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stripped = pd.DataFrame(stores_train_preprocessed).drop(columns=['revenue', 'lv1', 'busstop_importance_level', 'municipality_name', 'distance_from_busstop', 'district_municipality_name'])\n",
    "\"\"\" X_all = pd.DataFrame(stores_train_preprocessed.drop(columns='revenue')) \"\"\"\n",
    "y = stores_train_preprocessed['revenue']\n",
    "y_log = np.log10(y*10000) #Multiplying by 10 000 to avoid negative target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining and testing base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in range(1, 10):\n",
    "    \"\"\" tree_regressor_all = tree.DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    if tree_regressor_all.fit(X_all, y_log).tree_.max_depth < depth:\n",
    "        break\n",
    "    score_all=np.mean(cross_val_score(tree_regressor_all, X_all, y_log,\n",
    "                                  scoring = 'neg_mean_squared_log_error'))\n",
    "    score_all=math.sqrt(abs(score_all))\n",
    "    print('All: ', depth, score_all) \"\"\"\n",
    "    \n",
    "    tree_regressor_stripped = tree.DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    if tree_regressor_stripped.fit(X_stripped, y_log).tree_.max_depth < depth:\n",
    "        break\n",
    "    score_stripped=np.mean(cross_val_score(tree_regressor_stripped, X_stripped, y_log,\n",
    "                                  scoring = 'neg_mean_squared_log_error'))\n",
    "    score_stripped=math.sqrt(abs(score_stripped))\n",
    "    print('Stripped: ', depth, score_stripped) #Checking how well the base model was performing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR=GradientBoostingRegressor()\n",
    "search_grid = {'n_estimators':[50, 100, 200], 'learning_rate': [0.001, 0.01, 0.1],\n",
    "               'max_depth': [1, 2, 4], 'subsample': [0.5, 0.75, 1], 'random_state': [1]}\n",
    "search_all=GridSearchCV(estimator=GBR, param_grid=search_grid,\n",
    "                    scoring='neg_mean_squared_log_error')\n",
    "search_stripped=GridSearchCV(estimator=GBR, param_grid=search_grid,\n",
    "                    scoring='neg_mean_squared_log_error')\n",
    "\n",
    "# Using GridSearch to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" search_all.fit(X_all, y_log)\n",
    "print(search_all.best_params_)\n",
    "score_all = math.sqrt(abs(search_all.best_score_))\n",
    "print(\"All: \", score_all) \"\"\"\n",
    "\n",
    "search_stripped.fit(X_stripped, y_log)\n",
    "print(search_stripped.best_params_)\n",
    "score_stripped = math.sqrt(abs(search_stripped.best_score_))\n",
    "print(\"Stripped\", score_stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating actual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Best values from GridSearchCV\n",
    "GBR2_all = GradientBoostingRegressor(n_estimators=search_all.best_params_['n_estimators'], learning_rate=search_all.best_params_['learning_rate'],\n",
    "                                 subsample=search_all.best_params_['sub_sample'],max_depth=search_all.best_params_['max_depth'], random_state=1)\n",
    "score_all=np.mean(cross_val_score(GBR2_all, X_all, y_log, scoring='neg_mean_squared_log_error', n_jobs=1))\n",
    "score_all=math.sqrt(abs(score_all)) \"\"\"\n",
    "\n",
    "# Best values from GridSearchCV\n",
    "GBR2_stripped = GradientBoostingRegressor(n_estimators=search_stripped.best_params_['n_estimators'], learning_rate=search_stripped.best_params_['learning_rate'],\n",
    "                                 subsample=search_stripped.best_params_['sub_sample'],max_depth=search_stripped.best_params_['max_depth'], random_state=1)\n",
    "score_stripped=np.mean(cross_val_score(GBR2_stripped, X_stripped, y_log, scoring='neg_mean_squared_log_error', n_jobs=1))\n",
    "score_stripped=math.sqrt(abs(score_stripped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" GBR2_all.fit(X_all, y_log) \"\"\"\n",
    "GBR2_stripped.fit(X_stripped, y_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_test_ids = copy.deepcopy(stores_test_preprocessed['store_id']) #Saving IDs in the right format for submitting to Kaggle\n",
    "\n",
    "for row in range(0, len(stores_test_preprocessed)):\n",
    "    stores_test_preprocessed.iloc[row, 0] = int(stores_test_preprocessed.iloc[row, 0].replace('-', ''))\n",
    "\n",
    "stores_test_preprocessed['households_grunnkrets'] = stores_test_preprocessed['households_grunnkrets'].replace(np.nan, stores_test_preprocessed['households_grunnkrets'].mean())\n",
    "stores_test_preprocessed['grunnkrets_population'] = stores_test_preprocessed['grunnkrets_population'].replace(np.nan, stores_test_preprocessed['grunnkrets_population'].mean())\n",
    "stores_test_preprocessed['district_population'] = stores_test_preprocessed['district_population'].replace(np.nan, stores_test_preprocessed['district_population'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" prediction_all = (10**(GBR2_all.predict(stores_test_preprocessed.drop(columns='revenue'))))/10000 \"\"\"\n",
    "prediction_stripped = (10**(GBR2_stripped.predict(stores_test_preprocessed.drop(columns=['revenue', 'lv1', 'municipality_name', 'district_municipality_name', 'distance_from_busstop', 'busstop_importance_level']))))/10000\n",
    "\n",
    "\"\"\" data_all = {\"id\": stores_test_ids, \n",
    "        \"predicted\": prediction_all} \"\"\"\n",
    "\n",
    "data_stripped = {\"id\": stores_test_ids, \n",
    "        \"predicted\": prediction_stripped}\n",
    "\"\"\" \n",
    "prediction_all_submission = pd.DataFrame(data_all) \"\"\"\n",
    "prediction_stripped_submission = pd.DataFrame(data_stripped)\n",
    "\"\"\" \n",
    "prediction_all_submission.to_csv(\"./predictions/pred_all_150outliers_3feat.csv\", index=False) \"\"\"\n",
    "prediction_stripped_submission.to_csv(\"./predictions/pred_new_stripped_150outliers_3feat.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c18984bf529fdd2ca26e26b13f6f4be971b2555312a0b76fcbb57ae7a1603a3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
