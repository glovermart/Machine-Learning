{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Retail Revenue Prediction - Long notebook\n",
    "\n",
    "##### **Group Information**\n",
    "**Participants:**\n",
    "- Mina Chen Glein Feragen - 544552\n",
    "- Andrew Glover Marty - 557813\n",
    "- Simen Tvete Aabol - 505174\n",
    "\n",
    "\n",
    "**Kaggle Team name:** Group 8\n",
    "\n",
    "## Content:\n",
    "1. [Starting out](#starting-out)\n",
    "2. [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "    1. [Stores train](#stores_train)\n",
    "    2. [Plaace hierarchy](#Plaace-hierarchy)\n",
    "    3. [Grunnkrets Data](#Grunnkrets-Data)\n",
    "    4. [Extra Stores](#Extra-Stores)\n",
    "    5. [Household income](#Household-income)\n",
    "    6. [Buss](#Buss)\n",
    "    7. [Testing with mall_name](#Testing-with-mall_name)\n",
    "    7. [Testing with chain_name](#Testing-with-chain_name)\n",
    "    8. [Stores train](#Stores_train)\n",
    "    9. [Population](#Population)\n",
    "3. [Data Preprocessing](#Data-Preprocessing)\n",
    "4. [Feature Engineering](#Feature-Engineering)\n",
    "     1. [Population](#Population)\n",
    "     2. [Busstops](#Busstops)\n",
    "5. [Models](#models)\n",
    "    1. [Gradient Boosting Machine](#gradient-boosting-machine)\n",
    "6. [Results](#Results)\n",
    "6. [Reflections](#Reflections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting out\n",
    "\n",
    "When starting out with this project we first chose to spend some time on getting to know the data set and the task in front of us. We all looked through all the features of each table, and discussed to what degree we thought each feature would affect the revenue of a given store. We also brain stormed about possible algorithms to use for this project, for example random forest, light gradient boosting and extreme gradient boosting. \n",
    "\n",
    "After having this initial meeting, we started out with the EDA.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "For the EDA, we simply wanted to go through all of the given files, look for null values, outliers and correlation between features. We also wanted to check whether there were some errors or misleading information somewhere.\n",
    "\n",
    "We started out by simply importing all neccessary packages and data files, and then went through all files one by one.\n",
    "\n",
    "**Files:**\n",
    "- [stores_train.csv](#stores-train)\n",
    "- [plaace_hierarchy.csv](#plaace-hierarchy)\n",
    "- [grunnkrets_data.csv](#grunnkrets-data)\n",
    "- [stores_extra.csv](#extra-stores)\n",
    "- [grunnkrets_income_households.csv](#household-income)\n",
    "- [busstops_norway.csv](#buss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from plotly.offline import init_notebook_mode,iplot,plot\n",
    "init_notebook_mode(connected=True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "import hashlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stores Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started by getting an insight into the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "stores_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there is only one unique value in the 'year' column. We can therefore conclude that this value does not have a correlation with the column 'revenue'. We can therefore remove this column from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.pop(\"year\")\n",
    "stores_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that we have three columns where we are missing data, 'address', 'chain_name', and 'mall_name'.\n",
    "\n",
    "It is uncertain whether the column 'address' is necessary when we have geographical data in the form of latitude and longitude.\n",
    "'Address' and 'lat' + 'lon' represent the same data. We think this data will have the same same correlation/influence on 'revenue'. Thus, 'address' is removed because the column lacks a lot of data. There is also an advantage with 'lat' and 'lon' in that these are in a numerical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 'chain_name' and 'mall_name' we still have a problem with missing data.\n",
    "These two columns can have an impact on 'revenu'. For example, there may be exclusive restaurants with a c'hain_name' that have very high revenue. Or vice versa, if there are some chains that struggle a lot.\n",
    "\n",
    "It is often the easiest to delete rows where data is missing, but it is not appropriate in this situation when there is such a large proportion of the data set that is missing data. Ant we think these columns are important to have. \n",
    "\n",
    "The function of these two columns is to group the stores together.\n",
    "You can argue that you can remove the 'mall_name' column because we have the GPS locations of the stores. So you can let the algorithm(s) learn that many shops in one place can affect the 'revenue' column. But it's a lot easier for a model to understand categorical data. than coordinates that are close to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking closer at mall_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies = stores_train['mall_name'].value_counts()\n",
    "print(\"The count of each frequencies for the mall_name\\n\", absolute_frequencies.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Absolute frequencies: \\n\", absolute_frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The count of each frequencies\\n\", absolute_frequencies.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies.tail(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are many malls that only have 1 store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  stores_train.pop(\"mall_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stage:\n",
    "After going through many of the shopping centers on Google which, according to our data, only consist of one store, I see that the data is incomplete. Will therefore conclude that it will be better to only deal with GPS location in order to understand the geographical clustering of the stores. Therefore we drop the colum 'mall_namne'\n",
    "\n",
    "Later on:\n",
    "We are therefore thinking of removing the 'mall_name' column. But first, we will look further for a correlation. \n",
    "\n",
    " We have now gone back to 'mall_name' after becoming a little more familiar with cleaning data, and think that we can find a benefit in including the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crates a subset of stores_train where we remove all NaN-values. \n",
    "mall_subset = stores_train[stores_train['mall_name'].notna()]\n",
    "plt = mall_subset.groupby(['mall_name'])['revenue'].mean()\n",
    "\n",
    "plt.plot(kind='bar', title='Avrage revenue per mall', ylabel='Revenue', xlabel=\"Mall's\", figsize=(40, 5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are several malls that have higher revenue on average. Further tests to remove malls consisting of 2 or fewer stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3 # Anything that occurs less than this will be removed.\n",
    "for col in mall_subset.columns:\n",
    "    value_counts = mall_subset['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    mall_subset[col].replace(to_remove, np.nan, inplace=True)\n",
    "    \n",
    "plt = mall_subset.groupby(['mall_name'])['revenue'].mean()\n",
    "plt.plot(kind='bar', title='Avrage revenue per mall', ylabel='Revenue', xlabel=\"Mall's\", figsize=(40, 5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that several of the tall pillars have disappeared. Some of the spills/store chains that had an average revenue of over 120 have now been removed. These may be appropriate to remove to avoid overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 4 # Anything that occurs less than this will be removed.\n",
    "for col in mall_subset.columns:\n",
    "    value_counts = mall_subset['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    mall_subset[col].replace(to_remove, np.nan, inplace=True)\n",
    "    \n",
    "    \n",
    "plt = mall_subset.groupby(['mall_name'])['revenue'].mean()\n",
    "plt.plot(kind='bar', title='Avrage revenue per mall', ylabel='Revenue', xlabel=\"Mall's\", figsize=(40, 5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we set the threshold to 5, meaning that we only include malls that occur at least 5 times, we see a large difference in revenue.\n",
    "\n",
    "Also tests whether there is a relationship between not belonging to a mall and having something to say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_subset['mall_name'] = mall_subset['mall_name'].replace(np.nan, \"A-not a mall\")\n",
    "threshold = 6 # Anything that occurs less than this will be removed.\n",
    "for col in mall_subset.columns:\n",
    "    value_counts = mall_subset['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    mall_subset[col].replace(to_remove, \"A-not a mall\", inplace=True)\n",
    "    \n",
    "# print(\"sha3\", subset.shape)\n",
    "    \n",
    "plt = mall_subset.groupby(['mall_name'])['revenue'].mean()\n",
    "plt.plot(kind='bar', title='Avrage revenue per mall', ylabel='Revenue', xlabel=\"Mall's\", figsize=(40, 5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train['mall_name'] = stores_train['mall_name'].replace(np.nan, \"A-not a mall\")\n",
    "\n",
    "threshold = 4 # Anything that occurs less than this will be removed.\n",
    "for col in stores_train.columns:\n",
    "    value_counts = stores_train['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    stores_train[col].replace(to_remove, \"A-not a mall\", inplace=True)\n",
    "    \n",
    "plt = stores_train.groupby(['mall_name'])['revenue'].mean()\n",
    "plt.plot(kind='bar', title='Avrage revenue per mall', ylabel='Revenue', xlabel=\"Mall's\", figsize=(30, 5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train['mall_name'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check that the malls with one store have been converted to \"A-not in a mall\", the same as the NaN-values, we check if the mall \"Telegrafen\" still exist.\n",
    "Just to check that the malls with one store have been converted to \"A-not in a mall\", the same as the NaN-values, we check if the mall \"Telegrafen\" still exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = stores_train.loc[stores_train['mall_name'] == \"Telegrafen\"]                       \n",
    "print(temp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stage:\n",
    "\n",
    "'store_id' and 'store_name' represent the same data. We can therefore remove one of them. Here it is worth considering that 'store_id' is numerical, and this is not. 'store_name'.\n",
    "We can therefore remove 'store_name'. \n",
    "\n",
    "The same reasoning applies to 'plaace_hierarchy_id' and 'sales_channel_name'. Can therefore remove sales_channel_name'.\n",
    "\n",
    "Later on:\n",
    "Used these columns to test out a little different. Did they remove them not here, but somewhere else.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  stores_train.pop(\"store_id\")\n",
    "#  stores_train.pop(\"store_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking a little closer at 'grunnkrets_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies = stores_train['grunnkrets_id'].value_counts()\n",
    "print(\"The count of each frequencies for the grunnkrets_id\\n\", absolute_frequencies.tail(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are several 'grunnkrets_id' values that only have one store. On a small data set, this could be scary in terms of overfitting. Maybe something smart should be done here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be good to look through the values in 'revenu' to find out if there are any obvious outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.sort_values(by=['revenue'])['revenue'].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.sort_values(by=['revenue'])['revenue'].tail(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem like any of the values for revenue are outliers, of those with the highest revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute_frequencies = stores_train['revenue'].value_counts()\n",
    "sorted = stores_train.sort_values(by=['revenue'])\n",
    "absolute_frequencies = sorted['revenue'].value_counts()\n",
    "print(absolute_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems very strange. 217 stores had no income in 2016.Take a closer look at these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.sort_values(by=['revenue']).head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are suspiciously many stores that do not have more than '0.0' in revenue. May seem like shops that you have not had data on havd been given a default value of  '0.0' registered. Chooses to check up the store \"REMA 1000 TOLLNES\" and \"YX KJOS\" to confirm or deny the hypothesis.\n",
    "\n",
    "REMA 1000 TOLLNES was founded in 2005 and is still in operation. Found out that the store was active in 2016 through this article \"https://www.ta.no/det-gjor-meg-vondt-i-hjertet-a-kaste-mat/s/5-50-174104\". Unfortunately, I can't find anything about the store on \"www.proff.no\". \n",
    "\n",
    "YX KJOS was founded in 2010 and is still in operation. On the website \"https://www.regnskapstall.no/regnskapstall-for-kjos-servicesenter-as-103319245S1?view=full\" you can clearly see that the company had an income in 2016. So the data in the data set is wrong. The company had a revenue of 7,026,000, and a profit of 623,000.\n",
    "https://www.regnskapstall.no/regnskapstall-for-kjos-servicesenter-as-103319245S1?view=full\n",
    "\n",
    "We can therefore probably assume that most of the data that has the value '0.0' for 'revenue' is incorrect. Even a company that goes bankrupt has an income from a sale. It is probably the safest choice to remove these.  \n",
    "\n",
    "We can also confirm that the columns do not show a profit, as we found that one company both had a positive profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = copy.deepcopy(stores_train[stores_train.revenue != 0])\n",
    "sorted = stores_train.sort_values(by=['revenue'])\n",
    "absolute_frequencies = sorted['revenue'].value_counts()\n",
    "print(absolute_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check whether all the coordinates are in Norway, and look correct with a quick overview by printing all the coordinates on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBox = ( stores_train.lat.min(), stores_train.lat.max(), stores_train.lon.min(), stores_train.lon.max())\n",
    "print(BBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a bit of Googling online, we soon find out that these coordinates are within the borders of Norway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go \n",
    "import plotly.io as pio\n",
    "\n",
    "fig = px.density_mapbox(stores_train, lat='lat', lon='lon', radius=5,\n",
    "                        center=dict(lat=0, lon=180), zoom=0,\n",
    "                        mapbox_style='open-street-map')\n",
    "# It is possible to use mapbox_style=\"stamen-terrain\" \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that all the coordinates are in Norway.\n",
    "Now we can take a closer look at Norway if the distribution of stores makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_mapbox(stores_train, lat='lat', lon='lon', radius=1,\n",
    "                        center=dict(lat=65.5, lon=10), zoom=3,\n",
    "                        mapbox_style='open-street-map')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can intact with the map to explore more. To give an easy overview, the map shows that there are many shops in the big cities and fewer in the countryside. The coordinates in the dataset seem to make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes the columns that we assume are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.pop(\"sales_channel_name\")\n",
    "stores_train.pop(\"address\")\n",
    "stores_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking more closely at 'chain_name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train['chain_name'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train['chain_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train['chain_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we suspect that some data is missing. We will therefore see if we can generate the data we are missing in 'chain_name' through the data located in 'store_name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name_rema = stores_train[stores_train['store_name'].str.match('REMA')]\n",
    "store_name_rema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are 268 stores with names matching 'REMA'. Maybe someone is missing here. Since there are 269 registered under 'REMA FRANCHISE NORGE' in the 'chain_name' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_name_rema = stores_train[stores_train['chain_name'].str.match('REMA FRANCHISE NORGE',  na=False)]\n",
    "chain_name_rema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name_rema = chain_name_rema[~chain_name_rema[\"store_name\"].str.match('REMA 1000', na=False)]\n",
    "\n",
    "store_name_rema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that 'BRENDEN HANDEL' should not belong to 'REMA FRANCHISE NORGE'.\n",
    "\n",
    "\n",
    "Here we can create a code snippet to fix errors like this automatically. For this, to work there must be a clear link between the names in 'store_name' and 'chain_name'.\n",
    "\n",
    "After some looking at the data set, this does not seem entirely feasible. For example, 'chain_name'='3T' has the names '3 T ' and '3T'. Such things make it difficult to automate this process.\n",
    "\n",
    "It also gets complicated because several of the chain_names do not have a correlation with store_name. For example, there are many companies under the ALLIANCE OPTIKK chain, such as BRILLEHUSET HAMMERFEST, MIDT-TELEMARK SYNSSENTER, FRYDENLUND OPTIKK and OLLIS OPTIKK.\n",
    "On the other hand, if we had more time, we could go through it more manually. But since this is a very time-consuming process and because we do not have this time, we, therefore, choose not to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.sort_values(by=['chain_name']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing to fix what we assume was wrong with Rema 1000, also where we are closer to Kiwi, which we thought would be more widespread. Kiwi does not include in the top 5 stores measured in number. (REMA FRANCHISE NORGE, JOKER, MIX, CIRCLE K DETALJIST, BUNNPRIS) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name_rema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name_rema.at[7435, 'chain_name'] = np.nan \n",
    "store_name_rema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have now checked that this worked as desired. Therefore performs the action on stores_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.at[7435, 'chain_name'] = np.nan \n",
    "\n",
    "stores_train.query('store_id==\"915698204-915720811-778305\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain_name_rema = stores_train[stores_train['chain_name'].str.contains('REMA FRANCHISE NORGE',  na=False)]\n",
    "# chain_name_rema\n",
    "# store_name_rema = chain_name_rema[~chain_name_rema[\"store_name\"].str.contains('REMA 1000', na=False)]\n",
    "# store_name_rema\n",
    "# name_rema = stores_train[stores_train['store_name'].str.contains('REMA 1000',  na=False)]\n",
    "\n",
    "# Tried something with str.contains as first, but found str.match, whick better suitet this problem\n",
    "\n",
    "\n",
    "name_rema = stores_train[stores_train['store_name'].str.match('REMA')]\n",
    "# name_rema\n",
    "\n",
    "rema = name_rema[~name_rema[\"chain_name\"].str.contains('REMA', na=False)]\n",
    "\n",
    "rema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are no stores with names containing 'REMA' that are not in the REMA chain.\n",
    "\n",
    "Now we're going to take a closer look at Kiwi's stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_name_kiwi_norge = stores_train[stores_train['chain_name'].str.match('KIWI',  na=False)]\n",
    "chain_name_kiwi_norge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_name_kiwi_norge = stores_train[stores_train['chain_name'].str.match('KIWI NORGE',  na=False)]\n",
    "chain_name_kiwi_norge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested looking for 'KIWI' and 'KIWI NORWAY'. Finds exactly the same 64 rows. So then we know that the stores in the Kiwi chain have not been registered under different names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name_kiwi_norge = stores_train[stores_train['store_name'].str.contains('KIWI',  na=False)]\n",
    "store_name_kiwi_norge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can also see that at least one KIWI store is missing the chain value.\n",
    "\n",
    "Looking further into this, using the same procedure as with REMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name_kiwi = store_name_kiwi_norge[~store_name_kiwi_norge[\"chain_name\"].str.contains('KIWI', na=False)]\n",
    "store_name_kiwi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the one KIWI store that is not registered as a KIWI chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.at[8164, 'chain_name'] = \"KIWI NORGE\"\n",
    "\n",
    "stores_train.query('store_id==\"915526284-915802605-781854\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then checks whether there are any KIWI stores that are not registered as a KIWI chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_name_kiwi_norge = stores_train[stores_train['chain_name'].str.contains('KIWI',  na=False)]\n",
    "chain_name_kiwi_norge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name_kiwi = chain_name_kiwi_norge[~chain_name_kiwi_norge[\"store_name\"].str.contains('KIWI', na=False)]\n",
    "store_name_kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name_kiwi = chain_name_kiwi_norge[~chain_name_kiwi_norge[\"store_name\"].str.contains('KIWI', na=False)]\n",
    "store_name_kiwi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now no longer have use for 'stor_name', so remove this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.pop(\"store_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the index after some rows have been removed. This makes it easier to iterate through the data frame.\n",
    "\n",
    "Also checks if the dataframe is not a copy after this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = stores_train.reset_index(drop=True)\n",
    "stores_train._is_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking through the data in 'chain_name', we can now look for a correlation with 'revenue'. We do this in the same way as we did with 'mall_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_subset = stores_train\n",
    "# chain_subset = stores_train[stores_train['chain_name'].notna()]\n",
    "\n",
    "# mall_subset = stores_train[stores_train['mall_name'].notna()]\n",
    "# plt = mall_subset.groupby(['mall_name'])['revenue'].mean()\n",
    "\n",
    "\n",
    "\n",
    "chain_subset['chain_name'] = chain_subset['chain_name'].replace(np.nan, \"A-not a chain\")\n",
    "threshold = 0 # Anything that occurs less than this will be removed.\n",
    "for col in chain_subset.columns:\n",
    "    value_counts = chain_subset['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain_subset[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "        \n",
    "plt = chain_subset.groupby(['chain_name'])['revenue'].mean()\n",
    "plt.plot(kind='bar', title='Avrage revenue per chain_name', ylabel='Revenue', xlabel=\"Chain's\", figsize=(40, 5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that many chains do significantly better than others. Do as before to set up the minimum number of stores in a chain. \n",
    "\n",
    "We can also see that those who do not belong to a chain have a rather low average revenue of around 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = stores_train[stores_train['chain_name'].notna()]\n",
    "\n",
    "# chain_subset['chain_name'] = subset['chain_name'].replace(np.nan, \"AAA\")\n",
    "threshold = 3 # Anything that occurs less than this will be removed.\n",
    "for col in chain_subset.columns:\n",
    "    value_counts = chain_subset['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain_subset[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "        \n",
    "plt = chain_subset.groupby(['chain_name'])['revenue'].mean()\n",
    "plt.plot(kind='bar', title='Avrage revenue per chain_name', ylabel='Revenue', xlabel=\"Chain's\", figsize=(40, 5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried several different values for threshold to see if any of the columns that stand out disappeared. Seems like a threshold of 3 makes sense in this case.\n",
    "\n",
    "Therefore performing this on the data set we are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train['chain_name'] = stores_train['chain_name'].replace(np.nan, \"A-not a chain\")\n",
    "threshold = 0 # Anything that occurs less than this will be removed.\n",
    "for col in stores_train.columns:\n",
    "    value_counts = stores_train['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    stores_train[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "        \n",
    "plt = stores_train.groupby(['chain_name'])['revenue'].mean()\n",
    "plt.plot(kind='bar', title='Avrage revenue per chain_name', ylabel='Revenue', xlabel=\"Chain's\", figsize=(40, 5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes the values in 'chain_name' and 'mall_name' to numeric values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stage:\n",
    "\n",
    "# for i in range(len(stores_train) ):\n",
    "\n",
    "#     # if pd.isna( stores_train['chain_name'][i]) == False:\n",
    "#     sha = hashlib.sha3_256()\n",
    "#     value = stores_train['chain_name'][i]\n",
    "#     sha.update(value.encode('utf-8'))\n",
    "#     hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'big')\n",
    "\n",
    "#     stores_train.loc[i, 'chain_name'] = int(hashedInt)\n",
    "    \n",
    "#     # if pd.isna( stores_train['mall_name'][i]) == False:\n",
    "#     sha = hashlib.sha3_256()\n",
    "#     value = stores_train['mall_name'][i]\n",
    "#     sha.update(value.encode('utf-8'))\n",
    "#     hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'big')\n",
    "\n",
    "#     stores_train.loc[i,'mall_name'] = int(hashedInt)\n",
    "\n",
    "\n",
    "# Later on:\n",
    "\n",
    "stores_train['chain_name'] = LabelEncoder().fit_transform(stores_train['chain_name'])\n",
    "stores_train['mall_name'] = LabelEncoder().fit_transform(stores_train['mall_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes to numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stores_train.loc[5, 'chain_name'] )\n",
    "type(stores_train.loc[5, 'mall_name'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train['mall_name'] = pd.to_numeric(stores_train['mall_name'])\n",
    "stores_train['chain_name'] = pd.to_numeric(stores_train['chain_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stores_train.loc[5, 'chain_name'] )\n",
    "type(stores_train.loc[5, 'mall_name'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train['chain_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the number of unique values in the upper range matches the date we previously extracted.\n",
    "\n",
    "REMA FRANCHISE NORGE    269(removed one)\n",
    "JOKER                   164\n",
    "MIX                     115\n",
    "CIRCLE K DETALJIST      115\n",
    "BUNNPRIS                112\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plaace Hierarchy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will see if there is a correlation between \"plaace_hierarchy_id\" and \"revenue\", and if we find this we will look at the possibility of merging the data set \"plaace_hierarchy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Even if plt is imported earlier, this cell tends to crash unless plt is also imported here.\n",
    "\n",
    "fig = plt.figure()\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(20, 18), ncols=2)\n",
    "stores_train.groupby([\"plaace_hierarchy_id\"])['plaace_hierarchy_id'].count().plot(kind=\"barh\", ax=ax1)\n",
    "ax1.set_title('Number in each category')\n",
    "\n",
    "stores_train.groupby([\"plaace_hierarchy_id\"])['revenue'].mean().plot(kind=\"barh\", ax=ax2)\n",
    "ax2.set_title('Average revenue for each category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left side you can see the number of shops belonging to each category. On the right-hand side, we see the average revenue per store. We can see from this graph that there is a correlation. After a quick overview, you can see that \"3.3.3.0\" stands out. This category has many stores, and almost no revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read plaace_hierarchy data \n",
    "plaace_hierarchy = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "\n",
    "# Merge stores_train with information about the hierarchy\n",
    "stores_with_hierarchy = stores_train.merge(plaace_hierarchy, how='left', on='plaace_hierarchy_id')\n",
    "\n",
    "\n",
    "stores_with_hierarchy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_with_hierarchy['plaace_hierarchy_id'].equals(stores_with_hierarchy['lv4'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check to validate that the merge went as desired, and that the data in 'lv4' is equal to the 'plaace_hierarchy_id'.\n",
    "This is precisely the case when we get True returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stage:\n",
    "\n",
    "# for i in range(len(stores_with_hierarchy) ):\n",
    "    \n",
    "#     sha = hashlib.sha3_256()\n",
    "#     value = str(stores_with_hierarchy['lv1_desc'][i] + str(stores_with_hierarchy['lv1'][i]))\n",
    "#     sha.update(   (value.encode('utf-8'))     )\n",
    "#     hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'little')  # type: ignore\n",
    "#     stores_with_hierarchy.loc[i, 'lv1_desc'] = hashedInt\n",
    "    \n",
    "           \n",
    "#     sha = hashlib.sha3_256()\n",
    "#     value = str(stores_with_hierarchy['lv2_desc'][i] + str(stores_with_hierarchy['lv2'][i]))\n",
    "#     sha.update(   (value.encode('utf-8'))     )\n",
    "#     hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'little')  # type: ignore\n",
    "#     stores_with_hierarchy.loc[i, 'lv2_desc'] = hashedInt\n",
    "    \n",
    "#     sha = hashlib.sha3_256()\n",
    "#     value = str(stores_with_hierarchy['lv3_desc'][i] + str(stores_with_hierarchy['lv3'][i]))\n",
    "#     sha.update(   (value.encode('utf-8'))     )\n",
    "#     hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'little')  # type: ignore\n",
    "#     stores_with_hierarchy.loc[i, 'lv3_desc'] = hashedInt\n",
    "    \n",
    "#     sha = hashlib.sha3_256()\n",
    "#     value = str(stores_with_hierarchy['lv4_desc'][i] + str(stores_with_hierarchy['lv4'][i]))\n",
    "#     sha.update(   (value.encode('utf-8'))     )\n",
    "#     hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'little')  # type: ignore\n",
    "#     stores_with_hierarchy.loc[i, 'lv4_desc'] = hashedInt\n",
    "    \n",
    "# stores_with_hierarchy\n",
    "\n",
    "# Later on:\n",
    "stores_with_hierarchy['lv1_desc'] = LabelEncoder().fit_transform(stores_with_hierarchy['lv1_desc'])\n",
    "stores_with_hierarchy['lv2_desc'] = LabelEncoder().fit_transform(stores_with_hierarchy['lv2_desc'])\n",
    "stores_with_hierarchy['lv3_desc'] = LabelEncoder().fit_transform(stores_with_hierarchy['lv3_desc'])\n",
    "stores_with_hierarchy['lv4_desc'] = LabelEncoder().fit_transform(stores_with_hierarchy['lv4_desc'])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_with_hierarchy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several of the columns here contain the same information, so remove those that are string values and keep those that are numeric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_with_hierarchy.pop('lv1')\n",
    "stores_with_hierarchy.pop('lv2')\n",
    "stores_with_hierarchy.pop('lv3')\n",
    "stores_with_hierarchy.pop('lv4')\n",
    "# stores_with_hierarchy.pop('lv1_desc')\n",
    "# stores_with_hierarchy.pop('lv2_desc')\n",
    "# stores_with_hierarchy.pop('lv3_desc')\n",
    "# stores_with_hierarchy.pop('lv4_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grunnkrets Data\n",
    "We will now take a closer look at the data that deals with Norway's basic districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies = districts['year'].value_counts()\n",
    "absolute_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it can look like there is a large amount of duplicates. There are 26536 rows, and 13270 of these are dated from 2015, while 13266 of these are dated 2016. Note that these two add up to 26536, which is the number of rows we have.\n",
    "\n",
    "We can also observe that we have duplicates by seeing that they are 13270 unique values in 'grunnkrets_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_districts = districts.loc[districts['grunnkrets_id'] == 10020901]                       \n",
    "print(sub_districts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\t10020901\t2015\tTregde\tTregde-Skjernøy\tMandal  MULTIPOLYGON\n",
    "absolute_frequencies = sub_districts['geometry'].value_counts()\n",
    "# MULTIPOLYGON\n",
    "absolute_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_districts = districts.loc[districts['grunnkrets_id'] == 10030210]\n",
    "# POLYGON\n",
    "print(sub_districts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\t10030210\t2015\tBryneheia\tVanse/Åpta\tFarsund\tPOLYGON\n",
    "absolute_frequencies = sub_districts['geometry'].value_counts()\n",
    "# POLYGON\n",
    "absolute_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.pop(\"year\")\n",
    "districts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = districts.drop_duplicates()\n",
    "districts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that we are left with 13,270 rows. Originally it had 26,536 rows. This means that 13,266 rows have been removed out of the original 26,536."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = districts.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.loc[districts['grunnkrets_id'] == 10020901]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have two entries for this ID.\n",
    "\n",
    "Furthermore, we check whether there are still any duplicates on the ID. \"drop_duplicates()\" only removed the rows where entire rows are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts['grunnkrets_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that no 'grunnkrets_id' has been registered several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'grunnkrets_id' and 'grunnkrets_name' represent the same date. We can therefore remove one of these. Keep of the ID, as this is numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.pop('grunnkrets_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stage:\n",
    "Will preserve the values in 'district_name' and 'municipality_name'. Therefore hashes these into numerical values. Do not use the built-in hash function in python as this can in some cases return different output for the same input, on different machines.\n",
    "\n",
    "Before we start hashing these values, we check and analyze them in advance, so that we can see afterwards that everything went as desired.\n",
    "\n",
    "Later on:\n",
    "Change to use  LabelEncoder().fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts['district_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts['municipality_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts._is_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stage:\n",
    "\n",
    "It is  important that the dataframe is not a copy. That is, if 'districts.districts._is_copy' returns '<weakref .....>' then the code will crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(districts) ):\n",
    "#     sha = hashlib.sha3_256()\n",
    "#     value = districts['district_name'][i]\n",
    "#     sha.update(value.encode('utf-8'))\n",
    "#     hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'big')\n",
    "\n",
    "#     districts.loc[i, 'district_name'] = hashedInt\n",
    "\n",
    "\n",
    "\n",
    "districts['district_municipality_name'] = districts['district_name'] + '-'+ districts['municipality_name']\n",
    "districts.pop('district_name')\n",
    "districts['district_municipality_name'] = LabelEncoder().fit_transform(districts['district_municipality_name'])\n",
    "districts['municipality_name'] = LabelEncoder().fit_transform(districts['municipality_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts['municipality_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oslo         554\n",
    "Bærum        431\n",
    "Trondheim    429\n",
    "Bergen       361\n",
    "Stavanger    216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts['district_municipality_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentrum           123\n",
    "Bergen sentrum     47\n",
    "Konnerud           42\n",
    "Ås                 40\n",
    "Sandviken          37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the number of values representing 'Sentrum' is no longer the top. This is actually something we wanted to solve. From this we understand that there were several 'districts' in different municipalities that had the same district name. So, for example, 'Sentrum' in Oslo, and 'Sentrum' in Trondheim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will now check if there are any 'grunnkrets_id' entities that are not in 'stores_with_hierarchy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_from_districts = districts['grunnkrets_id']\n",
    "list_from_districts\n",
    "list_from_districts = list_from_districts.reset_index(drop=True)\n",
    "list_from_districts.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_from_stores_with_hierarchy = stores_with_hierarchy['grunnkrets_id']\n",
    "list_from_stores_with_hierarchy.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there exist 9481 'grunnkrets_id' that are not in the data set stores_train. \n",
    "This is not necessarily a problem. As there are many small places in Norway that do not have shops. It is possible that the number will decrease if we manage to bring in a few more from the data set 'stores_extra'.\n",
    "\n",
    "The deciding factor here is how large a proportion of the shops in the test-set are within these grunnkrets. \n",
    "\n",
    "Assumes 'store_train' does not contain 'grunnkrets_id' which does not exist in 'grundkrets_norway_stripped'. But have to check this out\n",
    "\n",
    "\n",
    "We later found that this assumption was wrong. It meant a little more work for us, but we managed to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_id = list_from_stores_with_hierarchy\n",
    "districts_id = list_from_districts\n",
    "\n",
    "\n",
    "\n",
    "stores_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_id = []\n",
    "for i in range(len(stores_id) ):\n",
    "    if stores_id[i] not in districts_id.values:\n",
    "        missing_id.append(stores_id[i])\n",
    "\n",
    "print( len(missing_id) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look  optimal. Here I think we have to generate data, or do something else smart.\n",
    "\n",
    "There are therefore 30 instances of 'grunnkrets_id' that have a shop, where we do not have data available from the data set 'grunnkrets_norway_stripped'\n",
    "\n",
    "This is a problem that can also arise when we have to test the data. I therefore think it would be appropriate to generate an automatic function that solves the problem. We can thus use this for the data when training and testing our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Stores \n",
    "The extra stores dataset is a collection of stores for which we had no revenue data. Structurally, it is identical to the test set, but you are naturally not expected to submit any predictions for it. You can, however, use the additional data in your analysis, in unsupervised methods you might employ, or to provide a stronger data basis for missing value imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_extra = pd.read_csv('data/stores_extra.csv')\n",
    "stores_extra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_extra.pop(\"year\")\n",
    "stores_extra.pop(\"sales_channel_name\")\n",
    "stores_extra.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_extra.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the data set has many of the same stores as we have in 'stores_train', but but data on 'chain_name' and 'mall_name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Do not remove the code - takes forever to run\n",
    "\n",
    "# match_id = []\n",
    "# stores_train_test = stores_train_test.reset_index(drop=True)\n",
    "\n",
    "# for i in range(len(stores_train_test) ):\n",
    "#     if stores_train_test['store_id'][i] in stores_extra.values:\n",
    "#         match_id.append(stores_extra[i])\n",
    "# print( 'Len: ', len(match_id) )\n",
    "# print( \"Id's \", (match_id) )\n",
    "        \n",
    "# this code returns 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the 'store_id's are repeated in the dataset. So can ignore the idea where we could add/update data.\n",
    "We will therefore not do any more work on/with this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the 'store_id's are repeated in the dataset. So can ignore the idea where we could add/update data.\n",
    "We will therefore not do any more work on/with this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this moment we dont need this code. But there is a possibility that we will need it later on. \n",
    "# Do not remove the code\n",
    "\n",
    "\n",
    "# for i in range(len(stores_with_hierarchy) ):\n",
    "    \n",
    " \n",
    "#     chain_int = int(stores_with_hierarchy['mall_name'][i])\n",
    "#     lv1_desc_int = int(stores_with_hierarchy['lv1_desc'][i])\n",
    "#     lv2_desc_int = int(stores_with_hierarchy['lv2_desc'][i])\n",
    "#     lv3_desc_int = int(stores_with_hierarchy['lv3_desc'][i])\n",
    "#     lv4_desc_int = int(stores_with_hierarchy['lv4_desc'][i])\n",
    "   \n",
    "#     stores_with_hierarchy.loc[i, 'lv1_desc'] = lv1_desc_int\n",
    "#     stores_with_hierarchy.loc[i, 'lv2_desc'] = lv2_desc_int\n",
    "#     stores_with_hierarchy.loc[i, 'lv3_desc'] = lv3_desc_int\n",
    "#     stores_with_hierarchy.loc[i, 'lv4_desc'] = lv4_desc_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this moment we dont need this code. But there is a possibility that we will need it later on. \n",
    "# Do not remove the code\n",
    "\n",
    "\n",
    "# stores_with_hierarchy['chain_name'] = pd.to_numeric(stores_with_hierarchy['chain_name'])\n",
    "# stores_with_hierarchy['mall_name'] = pd.to_numeric(stores_with_hierarchy['mall_name'])\n",
    "# stores_with_hierarchy['lv1_desc'] = pd.to_numeric(stores_with_hierarchy['lv1_desc'])\n",
    "# stores_with_hierarchy['lv2_desc'] = pd.to_numeric(stores_with_hierarchy['lv2_desc'])\n",
    "# stores_with_hierarchy['lv3_desc'] = pd.to_numeric(stores_with_hierarchy['lv3_desc'])\n",
    "# stores_with_hierarchy['lv4_desc'] = pd.to_numeric(stores_with_hierarchy['lv4_desc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Household income "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunnkrets_household_income = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "grunnkrets_household_income.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that we have many zero values. We therefore choose not to work further with these rows here. Therefore remove these.\n",
    "\n",
    "Here we can take some self-criticism for not spending more time going through the data. But with so many zero values, we assumed that we should rather spend our time on other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunnkrets_household_income.pop(\"singles\")\n",
    "grunnkrets_household_income.pop(\"couple_without_children\")\n",
    "grunnkrets_household_income.pop(\"couple_with_children\")\n",
    "grunnkrets_household_income.pop(\"other_households\")\n",
    "grunnkrets_household_income.pop(\"single_parent_with_children\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunnkrets_household_income.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies = grunnkrets_household_income['year'].value_counts()\n",
    "absolute_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grunnkrets_household_income.shape)\n",
    "income_sub = grunnkrets_household_income\n",
    "# income_sub.pop(\"year\")\n",
    "income_sub = income_sub.drop_duplicates()\n",
    "print(income_sub.shape)\n",
    "income_sub.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_sub = income_sub[income_sub.year != 2015]\n",
    "income_sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies = income_sub['grunnkrets_id'].value_counts()\n",
    "absolute_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chech some random samples that we know is the same district to analyse the income. \n",
    "11100\n",
    "12427\n",
    "6466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_sub.loc[income_sub['grunnkrets_id'] == 6466]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_sub.loc[income_sub['grunnkrets_id'] == 12427]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_sub.loc[income_sub['grunnkrets_id'] == 11100]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies = income_sub['all_households'].value_counts()\n",
    "absolute_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies = stores_with_hierarchy['district_municipality_name'].value_counts()\n",
    "absolute_frequencies\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_districts = pd.merge(income_sub,districts, how = 'right', on = 'grunnkrets_id')\n",
    "income_districts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_districts['grunnkrets_id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_districts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_districts.rename(columns = {'all_households':'households_grunnkrets'}, inplace = True)\n",
    "income_districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_districts.loc[income_districts['district_municipality_name'] == 64.0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have so far not used for 'geometry' and 'area_km2', therefore removing these.\n",
    "\n",
    "Realized we needed 'geometry' at a later stage to generate values in the columns we lacked data-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_districts.pop(\"geometry\")\n",
    "income_districts.pop(\"area_km2\")\n",
    "income_districts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_with_hierarchy = pd.merge(stores_with_hierarchy,income_districts, how = 'left', on = 'grunnkrets_id')\n",
    "stores_with_hierarchy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_with_hierarchy_and_income = stores_with_hierarchy\n",
    "stores_with_hierarchy_and_income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies = stores_with_hierarchy_and_income['district_municipality_name'].value_counts()\n",
    "absolute_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# income_districts['all_households_district'] = \n",
    "# income_districts_sub = income_districts\n",
    "income_per_district = stores_with_hierarchy_and_income.groupby(['district_municipality_name'])['households_grunnkrets'].mean()\n",
    "# income_districts = income_districts.reset_index(drop=True)\n",
    "# income_districts_sub\n",
    "# print( type(income_districts_sub) )\n",
    "# income_districts_sub.DataFrame(data = income_districts_sub) \n",
    "income_per_district\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Chech up this id: 100041825566693872077185226859240897274106223553590114601070335315116289585880 100041825566693872077185226859240897274106223553590114601070335315116289585880\n",
    "stores_with_hierarchy_and_income.loc[stores_with_hierarchy_and_income['district_municipality_name'] == 1539.0]  \n",
    "# income_districts.loc[income_districts['grunnkrets_id'] == \"10010701\"]  \n",
    "# income_districts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems very strange that all the constituencies within a district have the same income. Will have to look into this further.\n",
    "Choose to take some random samples of several of the 'grunnkrets_id' we see here. Implements these further up in the code.\n",
    "\n",
    "After this check, we had a strong suspicion that the data obtained in this table is actually made for districts and not grunnkrets.\n",
    "\n",
    "After reviewing the information we had about the date, we found that this was correct. It has thus not been necessary to group and calculate the average income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# income_districts.rename(columns = {'households':'households_grunnkrets'}, inplace = True)\n",
    "\n",
    "# Merge inn the district income into the store_train df\n",
    "# income_districts_all = pd.merge(income_districts,income_districts_sub, how = 'inner', on = 'district_municipality_name')\n",
    "# income_districts_all.head(25)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# stores_with_hierarchy = pd.merge(stores_with_hierarchy,income_districts, how = 'left', on = 'grunnkrets_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_districts.rename(columns = {'households_grunnkrets':'households_grunnkrets'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpoint to save time\n",
    "\n",
    "# SAVE\n",
    "# stores_with_hierarchy.to_csv('data/stores_with_hierarchy_copy.csv')\n",
    "# districts.to_csv('data/districts_copy.csv')\n",
    "# household_income.to_csv('data/household_income_copy.csv')\n",
    "# stores_with_hierarchy_and_income.to_csv('data/stores_with_hierarchy_and_income_copy.csv')\n",
    "\n",
    "\n",
    "# LOAD\n",
    "# stores_with_hierarchy = pd.read_csv('data/stores_with_hierarchy_copy.csv')\n",
    "# stores_with_hierarchy.pop('Unnamed: 0')\n",
    "# districts = pd.read_csv('data/districts_copy.csv')\n",
    "# districts.pop('Unnamed: 0')\n",
    "# household_income = pd.read_csv('data/household_income_copy.csv')\n",
    "# household_income.pop('Unnamed: 0')\n",
    "# stores_with_hierarchy_and_income = pd.read_csv('data/stores_with_hierarchy_and_income_copy.csv')\n",
    "# stores_with_hierarchy_and_income.pop('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss = pd.read_csv('data/busstops_norway.csv')\n",
    "buss.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it doesn't seem like 'side_placement' are of much interest here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies = buss['stopplace_type'].value_counts()\n",
    "absolute_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_frequencies = buss['importance_level'].value_counts()\n",
    "absolute_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'stopplace_type' doesn't look too interesting here either. 'importance_level', on the other hand, may be of interest. But it is therefore unfortunate that 55514 rows here have a 'Missing importance level', which can be considered missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buss.pop('geometry')\n",
    "buss.pop('stopplace_type')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could probably use 'stopplace_type', 'importance_level' and 'importance_level' to find a correlation between these. Then we could have tried to fill in the missing data. However, we chose not to set aside time to do this. This probably has something to do with the fact that we also didn't think it was worth the time in terms of what we wanted to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with mall_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = pd.read_csv('data/stores_train.csv')\n",
    "mall_subset = stores[['store_id', 'mall_name','revenue']]\n",
    "mall_subset['mall_null'] = ''\n",
    "mall_subset['mall_en'] = ''\n",
    "mall_subset['mall_to'] = ''\n",
    "mall_subset['mall_tre'] = ''\n",
    "mall_subset['mall_fire'] = ''\n",
    "mall_subset['mall_fem'] = ''\n",
    "mall_subset['mall_seks'] = ''\n",
    "mall_subset['hmall_null'] = ''\n",
    "mall_subset['hmall_en'] = ''\n",
    "mall_subset['hmall_to'] = ''\n",
    "mall_subset['hmall_tre'] = ''\n",
    "mall_subset['hmall_fire'] = ''\n",
    "mall_subset['hmall_fem'] = ''\n",
    "mall_subset['hmall_seks'] = ''\n",
    "mall_subset.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null\n",
    "mall_subset['mall_name'] = stores['mall_name'].replace(np.nan, \"A-not a mall\")\n",
    "\n",
    "threshold = 0 # Anything that occurs less than this will be removed.\n",
    "for col in mall_subset.columns:\n",
    "    value_counts = mall_subset['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    mall_subset[col].replace(to_remove, \"A-not a mall\", inplace=True)\n",
    "    \n",
    "mall_subset['mall_null'] = LabelEncoder().fit_transform(mall_subset['mall_name'])\n",
    "\n",
    "for i in range(len(mall_subset) ):\n",
    "    sha = hashlib.sha3_256()\n",
    "    value = mall_subset['mall_name'][i]\n",
    "    sha.update(value.encode('utf-8'))\n",
    "    hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'big')\n",
    "\n",
    "\n",
    "# en\n",
    "threshold = 1 # Anything that occurs less than this will be removed.\n",
    "for col in mall_subset.columns:\n",
    "    value_counts = mall_subset['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    mall_subset[col].replace(to_remove, \"A-not a mall\", inplace=True)\n",
    "    \n",
    "mall_subset['mall_en'] = LabelEncoder().fit_transform(mall_subset['mall_name'])\n",
    "\n",
    "for i in range(len(mall_subset) ):\n",
    "    sha = hashlib.sha3_256()\n",
    "    value = mall_subset['mall_name'][i]\n",
    "    sha.update(value.encode('utf-8'))\n",
    "    hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'big')\n",
    "\n",
    "    mall_subset.loc[i,'hmall_en'] = int(hashedInt)\n",
    "    \n",
    "\n",
    "#  to\n",
    "threshold = 2 # Anything that occurs less than this will be removed.\n",
    "for col in mall_subset.columns:\n",
    "    value_counts = mall_subset['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    mall_subset[col].replace(to_remove, \"A-not a mall\", inplace=True)\n",
    " \n",
    "mall_subset['mall_to'] = LabelEncoder().fit_transform(mall_subset['mall_name'])\n",
    "\n",
    "   \n",
    "for i in range(len(mall_subset) ):\n",
    "    sha = hashlib.sha3_256()\n",
    "    value = mall_subset['mall_name'][i]\n",
    "    sha.update(value.encode('utf-8'))\n",
    "    hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'big')\n",
    "\n",
    "    mall_subset.loc[i,'hmall_to'] = int(hashedInt)\n",
    "    \n",
    "    \n",
    "    \n",
    "# tre\n",
    "threshold = 3 # Anything that occurs less than this will be removed.\n",
    "for col in mall_subset.columns:\n",
    "    value_counts = mall_subset['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    mall_subset[col].replace(to_remove, \"A-not a mall\", inplace=True)\n",
    " \n",
    "mall_subset['mall_tre'] = LabelEncoder().fit_transform(mall_subset['mall_name'])\n",
    "\n",
    "   \n",
    "for i in range(len(mall_subset) ):\n",
    "    sha = hashlib.sha3_256()\n",
    "    value = mall_subset['mall_name'][i]\n",
    "    sha.update(value.encode('utf-8'))\n",
    "    hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'big')\n",
    "\n",
    "    mall_subset.loc[i,'hmall_tre'] = int(hashedInt)\n",
    "    \n",
    "    \n",
    "#  Fire    \n",
    "threshold = 4 # Anything that occurs less than this will be removed.\n",
    "for col in mall_subset.columns:\n",
    "    value_counts = mall_subset['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    mall_subset[col].replace(to_remove, \"A-not a mall\", inplace=True)\n",
    " \n",
    "mall_subset['mall_fire'] = LabelEncoder().fit_transform(mall_subset['mall_name'])\n",
    "\n",
    "   \n",
    "for i in range(len(mall_subset) ):\n",
    "    sha = hashlib.sha3_256()\n",
    "    value = mall_subset['mall_name'][i]\n",
    "    sha.update(value.encode('utf-8'))\n",
    "    hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'big')\n",
    "\n",
    "    mall_subset.loc[i,'hmall_fire'] = int(hashedInt)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Fem\n",
    "threshold = 5 # Anything that occurs less than this will be removed.\n",
    "for col in mall_subset.columns:\n",
    "    value_counts = mall_subset['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    mall_subset[col].replace(to_remove, \"A-not a mall\", inplace=True)\n",
    " \n",
    "mall_subset['mall_fem'] = LabelEncoder().fit_transform(mall_subset['mall_name'])\n",
    "\n",
    "   \n",
    "for i in range(len(mall_subset) ):\n",
    "    sha = hashlib.sha3_256()\n",
    "    value = mall_subset['mall_name'][i]\n",
    "    sha.update(value.encode('utf-8'))\n",
    "    hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'big')\n",
    "\n",
    "    mall_subset.loc[i,'hmall_fem'] = int(hashedInt)\n",
    "    \n",
    "    \n",
    "# seks\n",
    "threshold = 6 # Anything that occurs less than this will be removed.\n",
    "for col in mall_subset.columns:\n",
    "    value_counts = mall_subset['mall_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    mall_subset[col].replace(to_remove, \"A-not a mall\", inplace=True)\n",
    " \n",
    "mall_subset['mall_seks'] = LabelEncoder().fit_transform(mall_subset['mall_name'])\n",
    "\n",
    "   \n",
    "for i in range(len(mall_subset) ):\n",
    "    sha = hashlib.sha3_256()\n",
    "    value = mall_subset['mall_name'][i]\n",
    "    sha.update(value.encode('utf-8'))\n",
    "    hashedInt = int.from_bytes(hashlib.sha256(value.encode('utf-8')).digest(), 'big')\n",
    "\n",
    "    mall_subset.loc[i,'hmall_seks'] = int(hashedInt)\n",
    "    \n",
    "    \n",
    "mall_subset['hmall_null'] = pd.to_numeric(mall_subset['hmall_null'])\n",
    "mall_subset['hmall_en'] = pd.to_numeric(mall_subset['hmall_en'])\n",
    "mall_subset['hmall_to'] = pd.to_numeric(mall_subset['hmall_to'])\n",
    "mall_subset['hmall_tre'] = pd.to_numeric(mall_subset['hmall_tre'])\n",
    "mall_subset['hmall_fire'] = pd.to_numeric(mall_subset['hmall_fire'])\n",
    "mall_subset['hmall_fem'] = pd.to_numeric(mall_subset['hmall_fem'])\n",
    "mall_subset['hmall_seks'] = pd.to_numeric(mall_subset['hmall_seks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_only_numeric = mall_subset.drop(columns=[\"mall_name\"],axis=1)\n",
    "\n",
    "corr = data_only_numeric.corr()\n",
    "g, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.heatmap(corr, color=\"k\", annot=True, cmap=\"YlGnBu\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that there is actually a more linear correlation when you include all malls, even if they only consist of one store. Here we can see that the correlation is 0.025. This is obviously low, but still higher than the values we previously found.\n",
    "\n",
    "Now that we have found that all malls should be included, we will test whether a bool's value can be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_subset['mall_bool'] = stores['mall_name']\n",
    "mall_subset['mall_bool'] = mall_subset['mall_bool'].replace(np.nan, 0)\n",
    "\n",
    "for i in range(len(mall_subset) ):\n",
    "    if( mall_subset['mall_bool'][i] != 0):\n",
    "\n",
    "        mall_subset.loc[i,'mall_bool'] = 1\n",
    "        \n",
    "mall_subset['mall_bool'] = pd.to_numeric(mall_subset['mall_bool'])\n",
    "mall_subset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_only_numeric = mall_subset.drop(columns=[\"mall_name\"],axis=1)\n",
    "\n",
    "corr = data_only_numeric.corr()\n",
    "g, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.heatmap(corr, color=\"k\", annot=True, cmap=\"YlGnBu\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the Boolean value has a greater linear correlation with revenue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with chain_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stores train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = pd.read_csv('data/stores_train.csv')\n",
    "\n",
    "chain = stores[['store_id', 'chain_name','revenue']]\n",
    "\n",
    "chain['chain_null'] = ''\n",
    "chain['chain_en'] = ''\n",
    "chain['chain_to'] = ''\n",
    "chain['chain_tre'] = ''\n",
    "chain['chain_fire'] = ''\n",
    "chain['chain_fem'] = ''\n",
    "chain['chain_seks'] = ''\n",
    "chain['chain_syv'] = ''\n",
    "chain['chain_åtte'] = ''\n",
    "\n",
    "# Added after the first iteration\n",
    "chain['chain_null_bool'] = ''\n",
    "chain['chain_to_bool'] = ''\n",
    "chain['chain_fire_bool'] = ''\n",
    "chain['chain_seks_bool'] = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chain.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null\n",
    "chain['chain_name'] = stores['chain_name'].replace(np.nan, \"A-not a chain\")\n",
    "threshold = 0 # Anything that occurs less than this will be removed.\n",
    "for col in chain.columns:\n",
    "    value_counts = chain['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "       \n",
    "chain['chain_null'] = LabelEncoder().fit_transform(chain['chain_name'])\n",
    "\n",
    "chain['chain_null_bool'] = chain['chain_name']\n",
    "chain['chain_null_bool'] = chain['chain_null_bool'].replace(\"A-not a chain\", 0)\n",
    "\n",
    "for i in range(len(chain) ):\n",
    "    if( chain['chain_null_bool'][i] != 0):\n",
    "\n",
    "        chain.loc[i,'chain_null_bool'] = 1\n",
    "        \n",
    "chain['chain_null_bool'] = pd.to_numeric(chain['chain_null_bool'])\n",
    "\n",
    "\n",
    "\n",
    "# en\n",
    "threshold = 1 # Anything that occurs less than this will be removed.\n",
    "for col in chain.columns:\n",
    "    value_counts = chain['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "    \n",
    "chain['chain_en'] = LabelEncoder().fit_transform(chain['chain_name'])\n",
    "\n",
    "\n",
    "\n",
    "# to\n",
    "threshold = 2 # Anything that occurs less than this will be removed.\n",
    "for col in chain.columns:\n",
    "    value_counts = chain['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "    \n",
    "chain['chain_to'] = LabelEncoder().fit_transform(chain['chain_name'])\n",
    "\n",
    "chain['chain_to_bool'] = chain['chain_name']\n",
    "chain['chain_to_bool'] = chain['chain_to_bool'].replace(\"A-not a chain\", 0)\n",
    "\n",
    "for i in range(len(chain) ):\n",
    "    if( chain['chain_to_bool'][i] != 0):\n",
    "\n",
    "        chain.loc[i,'chain_to_bool'] = 1     \n",
    "chain['chain_to_bool'] = pd.to_numeric(chain['chain_to_bool'])\n",
    "\n",
    "\n",
    "\n",
    "# tre\n",
    "threshold = 3 # Anything that occurs less than this will be removed.\n",
    "for col in chain.columns:\n",
    "    value_counts = chain['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "    \n",
    "chain['chain_tre'] = LabelEncoder().fit_transform(chain['chain_name'])\n",
    "\n",
    "\n",
    "\n",
    "# fire\n",
    "threshold = 4 # Anything that occurs less than this will be removed.\n",
    "for col in chain.columns:\n",
    "    value_counts = chain['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "    \n",
    "chain['chain_fire'] = LabelEncoder().fit_transform(chain['chain_name'])   \n",
    "           \n",
    "chain['chain_fire_bool'] = chain['chain_name']\n",
    "chain['chain_fire_bool'] = chain['chain_fire_bool'].replace(\"A-not a chain\", 0)\n",
    "\n",
    "for i in range(len(chain) ):\n",
    "    if( chain['chain_fire_bool'][i] != 0):\n",
    "\n",
    "        chain.loc[i,'chain_fire_bool'] = 1\n",
    "        \n",
    "chain['chain_fire_bool'] = pd.to_numeric(chain['chain_fire_bool'])\n",
    "\n",
    "\n",
    "# fem\n",
    "threshold = 5 # Anything that occurs less than this will be removed.\n",
    "for col in chain.columns:\n",
    "    value_counts = chain['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "    \n",
    "chain['chain_fem'] = LabelEncoder().fit_transform(chain['chain_name'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# seks\n",
    "threshold = 6 # Anything that occurs less than this will be removed.\n",
    "for col in chain.columns:\n",
    "    value_counts = chain['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "    \n",
    "chain['chain_seks'] = LabelEncoder().fit_transform(chain['chain_name'])\n",
    "\n",
    "\n",
    "\n",
    "chain['chain_seks_bool'] = chain['chain_name']\n",
    "chain['chain_seks_bool'] = chain['chain_seks_bool'].replace(\"A-not a chain\", 0)\n",
    "\n",
    "for i in range(len(chain) ):\n",
    "    if( chain['chain_seks_bool'][i] != 0):\n",
    "\n",
    "        chain.loc[i,'chain_seks_bool'] = 1\n",
    "            \n",
    "chain['chain_seks_bool'] = pd.to_numeric(chain['chain_seks_bool'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chain['chain_fire_bool'] = chain['chain_name']\n",
    "chain['chain_fire_bool'] = chain['chain_fire_bool'].replace(\"A-not a chain\", 0)\n",
    "\n",
    "for i in range(len(chain) ):\n",
    "    if( chain['chain_fire_bool'][i] != 0):\n",
    "\n",
    "        chain.loc[i,'chain_fire_bool'] = 1\n",
    "        \n",
    "chain['chain_fire_bool'] = pd.to_numeric(chain['chain_fire_bool'])\n",
    "\n",
    "\n",
    "\n",
    "# syv\n",
    "threshold = 7 # Anything that occurs less than this will be removed.\n",
    "for col in chain.columns:\n",
    "    value_counts = chain['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "    \n",
    "chain['chain_syv'] = LabelEncoder().fit_transform(chain['chain_name'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# åtte\n",
    "threshold = 6 # Anything that occurs less than this will be removed.\n",
    "for col in chain.columns:\n",
    "    value_counts = chain['chain_name'].value_counts() # Specific column \n",
    "    to_remove = value_counts[value_counts <= threshold].index\n",
    "    chain[col].replace(to_remove, \"A-not a chain\", inplace=True)\n",
    "    \n",
    "chain['chain_åtte'] = LabelEncoder().fit_transform(chain['chain_name'])\n",
    "\n",
    "chain.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_only_numeric = chain.drop(columns=[\"chain_name\"],axis=1)\n",
    "\n",
    "corr = data_only_numeric.corr()\n",
    "g, ax = plt.subplots(figsize=(15,15))  \n",
    "sns.heatmap(corr, color=\"k\", annot=True, cmap=\"YlGnBu\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "grunnkrets = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "grunnkrets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunnkrets.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train.csv')\n",
    "grunnkrets = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "grunnkrets_ages = pd.read_csv('data/grunnkrets_age_distribution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concactenate population with other generated data\n",
    "population = get_population(stores_train, grunnkrets, grunnkrets_ages)\n",
    "population.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_population (stores_data, grunnkrets_data, grunnkrets_ages_data):\n",
    "    \"\"\" Returns population of grunnkrets and district\n",
    "        Manipulate or remove last line of code according to your taste :)\n",
    "        May require some more work for imputation of 'grunnkrets_population' using data from 'stores_extra' using 'lat' and 'lon'\n",
    "        Possibility of using unspervised learning?\n",
    "        Can be used for training only.Need to see how to make it multipurpose for test data as well.\n",
    "        Else Separate function is needed.\n",
    "    \"\"\"\n",
    "    #Get data for every grunnkrets_id and and drop duplicates. Prioritize the year '2016'\n",
    "    grunnkrets = grunnkrets.sort_values('year', ascending=False).drop_duplicates('grunnkrets_id').sort_index()\n",
    "    \n",
    "    #Create District+Municipality\n",
    "    grunnkrets_merged_district_municipality_name = grunnkrets\n",
    "    grunnkrets_merged_district_municipality_name['district_name_pro'] = grunnkrets['district_name'] + ' '+ grunnkrets['municipality_name']\n",
    "    \n",
    "    #Drop columns except 'grunnkrets_id' and 'district_municipality_name'\n",
    "    grunnkrets_with_district_municipality_names = grunnkrets_merged_district_municipality_name\n",
    "    grunnkrets_with_district_municipality_names = grunnkrets_with_district_municipality_names.drop(grunnkrets_with_district_municipality_names.iloc[:,1:5].columns,axis =1)\n",
    "    \n",
    "    grunnkrets_with_district_municipality_names = grunnkrets_with_district_municipality_names.drop('area_km2', axis =1)\n",
    "    \n",
    "    #Get data for every grunnkrets_id and and drop duplicates. Prioritize the year '2016'\n",
    "    grunnkrets_ages_new = grunnkrets_ages.sort_values('year', ascending=False).drop_duplicates('grunnkrets_id').sort_index()\n",
    "    \n",
    "    #Sum all ages in grunnkrets\n",
    "    grunnkrets_ages_new['population'] = grunnkrets_ages_new.iloc[:,2:].sum(axis =1)\n",
    "    \n",
    "    #Clean: Drop all age columns including year column\n",
    "    grunnkrets_population = grunnkrets_ages_new\n",
    "    grunnkrets_population = grunnkrets_population.drop(grunnkrets_population.iloc[:,1:93].columns,axis =1)\n",
    "    \n",
    "    grunnkrets_population_dist_muni = grunnkrets_population\n",
    "    grunnkrets_population_dist_muni = pd.merge(grunnkrets_with_district_municipality_names,grunnkrets_population, how = 'left', on = 'grunnkrets_id')\n",
    "    \n",
    "    grunnkrets_dist_muni = grunnkrets_population_dist_muni\n",
    "    grunnkrets_dist_muni = grunnkrets_dist_muni.drop(['population'], axis = 1)\n",
    "    \n",
    "    #Merge only grunnkrets population and find missing population before merging\n",
    "    merge_grunnkrets_populn_stores_train = pd.merge(stores_train,grunnkrets_dist_muni, how = 'left', on = 'grunnkrets_id')\n",
    "    \n",
    "    #Add population of district\n",
    "    st_train_grunn_pp = pd.merge(merge_grunnkrets_populn_stores_train,grunnkrets_population, how = 'left', on = 'grunnkrets_id')\n",
    "    \n",
    "    #Get Population of District\n",
    "    population_dist_muni = st_train_grunn_pp\n",
    "    population_dist_muni = population_dist_muni.groupby('district_name_pro')['population'].sum()\n",
    "    \n",
    "    #Merge\n",
    "    st_train_grunn_pp_dist_pp = pd.merge(st_train_grunn_pp,population_dist_muni, how = 'left', on = 'district_name_pro')\n",
    "    \n",
    "    st_train_grunn_pp_dist_pp.rename(columns = {'population_x':'grunnkrets_population','population_y': 'district_population'}, inplace = True)\n",
    "    \n",
    "    st_train_grunn_pp_dist_pp['geometry'] = gpd.GeoSeries.from_wkt(st_train_grunn_pp_dist_pp['geometry'])\n",
    "\n",
    "    store_gdf = gpd.GeoDataFrame(st_train_grunn_pp_dist_pp, geometry='geometry')\n",
    "    store_gdf = store_gdf.drop_duplicates()\n",
    "    \n",
    "    for index, row in st_train_grunn_pp_dist_pp.iterrows():\n",
    "        # print(\"row\", row)\n",
    "    \n",
    "        if pd.isnull(row['grunnkrets_population']):\n",
    "            lat = row['lat']\n",
    "            lon = row['lon']\n",
    "            \n",
    "            store_location = Point(lon, lat)\n",
    "        \n",
    "            polygon_indices = store_gdf.distance(store_location).sort_values().index[0:150] #lower values returns missing data for grunnkrets_population\n",
    "            #cannot guarantee accuracy of imputed missing population\n",
    "            nearest_grunnkretser = store_gdf.loc[polygon_indices]\n",
    "            \n",
    "            st_train_grunn_pp_dist_pp['grunnkrets_population'].loc[index] = np.floor(nearest_grunnkretser['grunnkrets_population'].mean())\n",
    "        \n",
    "    for index, row in st_train_grunn_pp_dist_pp.iterrows():        \n",
    "        if pd.isnull(row['district_population']):\n",
    "            lat = row['lat']\n",
    "            lon = row['lon']\n",
    "            \n",
    "            store_location = Point(lon, lat)\n",
    "        \n",
    "            polygon_indices = store_gdf.distance(store_location).sort_values().index[0:4]\n",
    "            nearest_grunnkretser = store_gdf.loc[polygon_indices]\n",
    "            \n",
    "            st_train_grunn_pp_dist_pp['district_population'].loc[index] = np.floor(nearest_grunnkretser['district_population'].mean())\n",
    "    \n",
    "    st_train_grunn_pp_dist_pp = st_train_grunn_pp_dist_pp.drop(st_train_grunn_pp_dist_pp.iloc[:,1:14].columns,axis =1)#Take off this if all colmuns are needed\n",
    "    return st_train_grunn_pp_dist_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busstops\n",
    "##### Add column with distance from closest bus stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the busstops we wanted to calculate the distance from each store to the closest busstop, as we thought this would be an interesting feature. We also wanted to keep the importance levels of the busstops, as a more important busstop nearby will be more valuable for a store than a not so often used busstop.\n",
    "\n",
    "We started out by importing the file we were given, and dropped the columns we found less helpful during the EDA, i.e. stopplace_type and side_placement.\n",
    "\n",
    "After that we replaced the categorical values with numerical ones, and inserted columns in the table for latitude and longditude, so that it would be easier to compute the distance (as opposed to using the \"geometry\" of the busstops). We then dropped geometry, since we had the relevant information saved in other columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busstops = pd.read_csv('./data/busstops_norway.csv')\n",
    "busstops = busstops.drop(columns=[\"stopplace_type\", \"side_placement\"])\n",
    "\n",
    "importance_levels = [\"Mangler viktighetsnivå\", \"Standard holdeplass\", \"Lokalt knutepunkt\",\n",
    "                     \"Regionalt knutepunkt\", \"Annen viktig holdeplass\", \"Nasjonalt knutepunkt\"]\n",
    "numerated_importance_levels = [1, 2, 4, 5, 3, 6]\n",
    "\n",
    "busstops[\"importance_level\"] = busstops[\"importance_level\"].replace(importance_levels, numerated_importance_levels)\n",
    "\n",
    "busstops.insert(3, \"lat\", -math.inf)\n",
    "busstops.insert(4, \"lon\", -math.inf)\n",
    "\n",
    "busstops_array = []\n",
    "for row_index in range(len(busstops)):\n",
    "    coordinates = busstops[\"geometry\"][row_index][6:-2].split(' ')\n",
    "    busstops[\"lon\"][row_index] = float(coordinates[0])\n",
    "    busstops[\"lat\"][row_index] = float(coordinates[1])\n",
    "    busstops_array.append([float(coordinates[1]), float(coordinates[0])])\n",
    "busstops = busstops.drop(columns=\"geometry\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we find the distance to the closest busstop, and return the given distance and its importance level. This function can so be used to add another column to the store data, in order to know distance to the closest bus stop, and the importance of the bus stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistFromBusStop(store_lat, store_lon):\n",
    "    busstops_big_array = np.asarray(busstops_array)\n",
    "    distances = np.sum((busstops_big_array-[store_lat, store_lon])**2, axis=1)\n",
    "    index_busstop = np.argmin(distances)\n",
    "    shortest_distance = distances[index_busstop]\n",
    "    return shortest_distance, busstops.at[index_busstop, \"importance_level\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we wanted a feature that combined these to features, which we ended up simply calling \"buss\". This is the log10 value of the product of the distance and importance level. The reason we wanted to use the log, instead of the actual value, was that we plotted the feature importance before and after applying the log10 to the buss-feature, and the importance improved with the log10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWeightedBusData(df):\n",
    "    for index, row in df.iterrows():\n",
    "        # Adding distance to closest busstop\n",
    "        distance, importance_level = getDistFromBusStop(row['lat'], row['lon'])\n",
    "        df.at[index, \"distance_from_busstop\"] = distance\n",
    "        df.at[index, \"busstop_importance_level\"]= importance_level\n",
    "        \n",
    "    df.insert(11, 'buss', -math.inf)\n",
    "    df['buss'] = np.log10(np.multiply(df['distance_from_busstop'], df['busstop_importance_level']))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with random forest quite early on. This was an algorithm we had heard about before. Our thought was that it was important to familiarize yourself with how to work with such alogorithms, so we therefore chose to test this one. This is a good lesson for us. We had intended this to be a simple algorithm, so that's why we thought this was a good start for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "stores_train = pd.read_csv('data/stores_train_preprocessed.csv')\n",
    "stores_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = stores_train[stores_train.revenue > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = stores_train[stores_train.revenue > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = stores_train.drop(['store_id'], axis = 1) \n",
    "stores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train['grunnkrets_population'] = (stores_train['grunnkrets_population'].fillna(stores_train.grunnkrets_population.mean()))\n",
    "stores_train['district_population'] = (stores_train['district_population'].fillna(stores_train.district_population.mean()))\n",
    "stores_train['area_km2'] = (stores_train['area_km2'].fillna(stores_train.area_km2.mean()))\n",
    "stores_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stores_train\n",
    "Y= data[['revenue']]\n",
    "Y=np.ravel(Y)\n",
    "X= data.drop('revenue', axis =1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(random_state = 42,\n",
    "                                  n_jobs =-1,\n",
    "                                   n_estimators = 3000,\n",
    "                                 )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = regressor.score(X,Y)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Variable':X.columns,\n",
    "              'Importance':regressor.feature_importances_}).sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_test',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = regressor.score(X_train,y_train)\n",
    "score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = regressor.score(X_train,y_train)\n",
    "score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = regressor.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rmse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_non_log = 10**(y_pred)\n",
    "y_test_non_log = 10**(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "rmsle = mean_squared_log_error(y_test_non_log,y_pred_non_log)**0.5\n",
    "rmsle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test_non_log))\n",
    "f = plt.figure()\n",
    "f.set_figwidth(30)\n",
    "f.set_figheight(5)\n",
    "plt.plot(x_ax, y_test_non_log, label=\"truth\")\n",
    "plt.plot(x_ax, y_pred_non_log, label=\"predicted\")\n",
    "plt.title(\"Truth vs predicted Revenue\")\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend(loc='best',fancybox=True, shadow=True)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started working with this algorithm after Random forest. We started with this in parallel with LightGBM. These two are more complex compared to the Random front, and were a natural choice. Here we had become better acquainted with ML, and which algorithms existed. After some searching online to find more information, we chose to test Catboost and LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train_preprocessed.csv')\n",
    "# stores_train = pd.read_csv('data/stores_train.csv')\n",
    "stores_train['grunnkrets_population'] = stores_train['grunnkrets_population'].fillna(stores_train.grunnkrets_population.mean())\n",
    "stores_train['district_population'] = stores_train['district_population'].fillna(stores_train.district_population.mean())\n",
    "stores_train['area_km2'] = stores_train['area_km2'].fillna(stores_train.area_km2.mean())\n",
    "stores_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.boxplot(stores_train['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = stores_train[stores_train.revenue > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stores_train\n",
    "data = data.drop('store_id',\n",
    "                   axis =1)\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= data.pop(\"revenue\")\n",
    "X= data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)\n",
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_test',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = CatBoostRegressor( iterations= 5000, random_seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(cat_model.get_feature_importance(prettified=True))\n",
    "\n",
    "plt.figure(figsize=(12, 6));\n",
    "feature_plot= sns.barplot(x=\"Importances\", y=\"Feature Id\", data = feature_importance,palette=\"cool\");\n",
    "plt.title('feature importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and validate\n",
    "cat_model.fit( X_train, y_train,\n",
    "               eval_set=(X_test, y_test),\n",
    "               plot=True,\n",
    "              verbose = False\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = cat_model.predict(X_train)\n",
    "y_predict= cat_model.predict(X_test)\n",
    "#RMSE\n",
    "Rmse_train = math.sqrt(mean_squared_error(y_train,y_predict_train ))\n",
    "Rmse_test = math.sqrt(mean_squared_error(y_test,y_predict))\n",
    "\n",
    "#R\n",
    "r2_train = cat_model.score(X_train,y_train)\n",
    "r2_test = r2_score(y_test,y_predict)\n",
    "\n",
    "# Adjusted R2 \n",
    "n= X_train.shape[0] \n",
    "p= X_train.shape[1] \n",
    "adj_r2_test = 1-(1-r2_test)*(n-1)/(n-p-1)\n",
    "\n",
    "print(\"Evaluation on test data\")\n",
    "print(\"RMSE train: {:.2f}\".format(Rmse_train))\n",
    "print(\"RMSE test: {:.2f}\".format(Rmse_test))\n",
    "print(\"R2 train: {:.2f}\".format(r2_train))\n",
    "print(\"R2 test: {:.2f}\".format(r2_test))\n",
    "print(\"Adjusted R2: {:.2f}\".format(adj_r2_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test))\n",
    "f = plt.figure()\n",
    "f.set_figwidth(30)\n",
    "f.set_figheight(5)\n",
    "plt.plot(x_ax, y_test, label=\"truth\")\n",
    "plt.plot(x_ax, y_predict, label=\"predicted\")\n",
    "plt.title(\"Truth vs predicted Revenue\")\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend(loc='best',fancybox=True, shadow=True)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_non_log = 10**(y_predict) \n",
    "y_test_non_log = 10**(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "rmsle = mean_squared_log_error(y_test_non_log,y_pred_non_log)**0.5\n",
    "rmsle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test_non_log))\n",
    "f = plt.figure()\n",
    "f.set_figwidth(30)\n",
    "f.set_figheight(5)\n",
    "plt.plot(x_ax, y_test_non_log, label=\"truth\")\n",
    "plt.plot(x_ax, y_pred_non_log, label=\"predicted\")\n",
    "plt.title(\"Truth vs predicted Revenu\")\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend(loc='best',fancybox=True, shadow=True)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machine\n",
    "Another algorithm we tried out was gradient boosting, namely the GradientBoostingRegressor by SciKit-Learn. The reason we chose to use this library, is because it was the one we had heard the most of, and been recommended by peers and mentors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining and testing base model \n",
    "We first started out by defining and testing a base model, which was a simple tree, also from SciKit-Learn, just to get an idea of approximately what performance we could expect from the model. To make it even more predictable, we used the same evaluation metric as the Kaggle Leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(stores_train.drop(columns=\"revenue\"))\n",
    "y = pd.DataFrame(stores_train[\"revenue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in range(1, 10):\n",
    "    tree_regressor = tree.DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    if tree_regressor.fit(X, y).tree_.max_depth < depth:\n",
    "        break\n",
    "    score_all=np.mean(cross_val_score(tree_regressor, X, y,\n",
    "                                  scoring = 'neg_mean_squared_log_error'))\n",
    "    score_all=math.sqrt(abs(score_all))\n",
    "    print(depth, score_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we got were already very good, and so we continued on with hyperparameter tuning.\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "For the hyperparameter tuning we used used an actual Gradient Boosting Regressor (GBR), and tuned the model via Grid Search. We tried with the following values:\n",
    "- n_estimators: 5, 10, 20, 50, 100, 200\n",
    "- learning_rate: 0.001, 0.01, 0.1\n",
    "- max_depth: 1, 2, 4\n",
    "- subsample: 0.5, 0.75, 1\n",
    "\n",
    "Because of running time, we did not run the entire thing as one, but rather with different combinations of the n_estimators. Either way, this was how we did it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR=GradientBoostingRegressor()\n",
    "search_grid = {'n_estimators':[50, 100, 200], 'learning_rate': [0.001, 0.01, 0.1],\n",
    "               'max_depth': [1, 2, 4], 'subsample': [0.5, 0.75, 1], 'random_state': [1]}\n",
    "search=GridSearchCV(estimator=GBR, param_grid=search_grid,\n",
    "                    scoring='neg_mean_squared_log_error')\n",
    "search.fit(X, y)\n",
    "print(search.best_params_)\n",
    "score = math.sqrt(abs(search.best_score_))\n",
    "print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to trying with these different hyperparameters, we also tried out with different features (i.e. dropping some of the features we found were not so relevant), tried normalizing the revenue, doing nothing with it, and applying the log10. When it comes to the revenue, we found that the latter performed the best each time for this model. To save you from reading unneccessary many versions of the same code, we will keep only one in this notebook, but in short, we found that:\n",
    "- doing nothing to the revenue performed better than normalizing the revenue. Applying the log10 performed better than doing nothing.\n",
    "- \"Stripping\" the DataFrame of certain columns was better than keeping all. We will talk more about this later, but we tried out multiple possibilities, based on correlation to revenue, and based on feature importance\n",
    "- A max depth of 4 and learning rate of 0.1 seemed to be the best each time\n",
    "- For \"stripped\" DataFrames, a subsample of 0.75 seemed to perform better than a subsample of 1, even though a subsample of 1 seemed to perform better on non-\"stripped\" DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating actual models\n",
    "After doing some trying and failing, we used what the GridSearch told us to use, and fitted the regressor with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR2 = GradientBoostingRegressor(n_estimators=search.best_params_['n_estimators'], learning_rate=search.best_params_['learning_rate'],\n",
    "                                 subsample=search.best_params_['sub_sample'],max_depth=search.best_params_['max_depth'], random_state=1)\n",
    "score=np.mean(cross_val_score(GBR2, X, y, scoring='neg_mean_squared_log_error', n_jobs=1))\n",
    "score=math.sqrt(abs(score_all))\n",
    "print(score)\n",
    "GBR2.fit(X, y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions\n",
    "We then used the model to make predictions, one of which is our end submission (pred_new_stripped_150outliers_3feat). Here we chose to remove the top and bottom 150 stores based on revenue in order to make the data more representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = GBR2.predict(stores_test)\n",
    "data = {'id': stores_test['store_id'],\n",
    "        'predicted': prediction}\n",
    "prediction_submission = pd.DataFrame(data)\n",
    "prediction_submission.to_csv(\"./predictions/pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first predictions we also used the performance of these to improve on our next predictions. We did this by plotting the feature importance, which later indicated which features to use and not for the \"stripped\" DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\n",
    "feat_importance = GBR2.feature_importances_\n",
    "feat_importance = 100.0 * (feat_importance / feat_importance.max())\n",
    "sorted_index = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_index.shape[0]) + .5\n",
    "plt.figure(figsize=(8, 18))\n",
    "plt.barh(pos, feature_importance[sorted_index], align='center')\n",
    "plt.yticks(pos, X.keys()[sorted_index])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the chain_name and store_id is very important, and we therefore never tried to take them out. The lv1 and municipality name however were not so important, and therefore we tried making predictions without those and other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The file for our prediction with score 0.69378 :**\n",
    "- [short_nootebook_pred_new_stripped_150outliers_3feat.csv](#short_nootebook_pred_new_stripped_150outliers_3feat.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The file for our prediction with score____ :**\n",
    "- [______.csv](#______.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has been a very educational project. We have learned a lot. None of us has worked with machine learning to any great extent before. We therefore had a lot of theory that we had to familiarize ourselves with.\n",
    "\n",
    "This process went well but caused things to go rather slowly at first. We consider this to be understandable, as a maturation period is often needed when working with something completely new.\n",
    "\n",
    "We were a team that did not know each other from before, and who worked remotely. This has probably affected the work process a little, but all in all, we are satisfied with how it has gone.\n",
    "\n",
    "At the start, much of the time was spent figuring out what to do, as we had no experience with ML. If we had started the project anew with our current knowledge, we would probably have had time to try out a few more things. Such as, among other things:\n",
    "\n",
    "- We could try to generate chain_name ourselves based on store_name. Alternatively, we could try to fill in the chain_name column with the data we had available.\n",
    "\n",
    "- The column mall_name probably also had missing data. Here we could try to generate this data ourselves by looking at the coordinates. We could test putting all stores that were within a certain distance from each other in the same mall_name.\n",
    "\n",
    "- We did not use the dataset store_extra to a particularly large extent, here we could possibly have spent more time on theory on ML and perhaps found an area of ​​use for this data.\n",
    "\n",
    "- We could obviously spend more time on models, but this in itself is very time-consuming, and it is a separate field to work on perfecting parameters.\n",
    "\n",
    "But despite what we could have done, we are very satisfied with what we were able to do. We got a good insight into the data through our EDA. We went through the stores_train dataset in particular very accurately. Here, among other things, we got verified data, to the extent that the data we found looked correct.\n",
    "\n",
    "- Like, among other things, there were many grocery stores that had high incomes. This represents the news picture well.\n",
    "\n",
    "- We went through the types of stores that existed, and the income for these categories.\n",
    "\n",
    "- We got confirmation that the coordinates for the store made sense, and represented the whole of Norway. Here we could clearly see that the shops were in the majority of the big cities, which makes sense.\n",
    "\n",
    "- We also went through and tested how well chain_names were set based on store_name. This was done surprisingly well. Here we spent a lot of time at the grocery stores Rema 1000 and Kiwi in particular.\n",
    "\n",
    "- We also went in-depth when it came to income. There were 217 rows that had an income =0. Here we discussed whether it was actually a profit that had been obtained. But since no stores had negative values, this was hard to believe. Unfortunately, not all stores go into surplus. We also checked the stores and found out after a bit of Googling that they were active and had both positive income and profit in 2016. We therefore chose to remove these rows. We therefore make a 'feature selection' here.\n",
    "\n",
    "- Furthermore, we are very satisfied with our 'feature engineering/creations' with the bus stops and the population.\n",
    "\n",
    "- Another thing we are pleased with is how we generated the data we were missing. This is because we did not have data for all the grunnkrets. We did this by calculating an average from the nearest basic districts, geographically.\n",
    "\n",
    "- Using 'model interpretation' for the models catboost and AutoML helped us a lot in interpreting our weak results. This helped us find a bug in our preprocessing. We preprocessed stores_train and stores_test separately. This in itself is normal, but we managed to make a big mistake. We created new instances of 'LabelEncoder().fit_transform()' for each dataset. This resulted in our categorized data being represented differently. An example of this could be the chain_name value 'REMA FRANCHISE NORGE' can be converted to '7' in stores_train and a '9' in stores_test. Naturally, these two should have been converted to the same value for a model to benefit from the chain_name column.  \n",
    "Our model interpretation clearly understood that it was 'chain_name' that was most important, ie had the biggest correlation with 'revenue'. This fits well with our previous findings using sns.heatmap, which calculated that there is a 0.39/0.40 linear correlation with the target attribute 'revenue'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c18984bf529fdd2ca26e26b13f6f4be971b2555312a0b76fcbb57ae7a1603a3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
